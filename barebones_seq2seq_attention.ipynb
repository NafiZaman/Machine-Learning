{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35e56939-225a-4973-82b6-7361df32770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0f4cd3-60d6-4a9e-b70e-c9ac166b403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 4\n",
    "source_seq_len = 6\n",
    "target_seq_len = 7\n",
    "batch_size = 2\n",
    "source_vocab_size = 9\n",
    "target_vocab_size = 10\n",
    "hidden_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769184d-7ed7-4130-a85b-9b92945cdfc5",
   "metadata": {},
   "source": [
    "## Embedding the SOURCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad2e5242-d8d5-4105-a47d-aa24646880c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source vector:\n",
      "tensor([[2, 2],\n",
      "        [1, 5],\n",
      "        [3, 6],\n",
      "        [4, 7],\n",
      "        [0, 8],\n",
      "        [0, 4]])\n",
      "\n",
      "source_embedded: torch.Size([6, 2, 4])\n",
      "tensor([[[-0.7121,  0.3037, -0.7773, -0.2515],\n",
      "         [-0.7121,  0.3037, -0.7773, -0.2515]],\n",
      "\n",
      "        [[-0.1002, -0.6092, -0.9798, -1.6091],\n",
      "         [ 0.1991,  0.0457,  0.1530, -0.4757]],\n",
      "\n",
      "        [[-0.2223,  1.6871,  0.2284,  0.4676],\n",
      "         [-1.8821, -0.7765,  2.0242, -0.0865]],\n",
      "\n",
      "        [[-0.6970, -1.1608,  0.6995,  0.1991],\n",
      "         [ 2.3571, -1.0373,  1.5748, -0.6298]],\n",
      "\n",
      "        [[-1.5256, -0.7502, -0.6540, -1.6095],\n",
      "         [ 2.4070,  0.2786,  0.2468,  1.1843]],\n",
      "\n",
      "        [[-1.5256, -0.7502, -0.6540, -1.6095],\n",
      "         [-0.6970, -1.1608,  0.6995,  0.1991]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "source_vector = torch.tensor([[2, 2], [1, 5], [3, 6], [4, 7], [0, 8], [0, 4]])\n",
    "print(\"source vector:\")\n",
    "print(source_vector)\n",
    "\n",
    "embed = nn.Embedding(source_vocab_size, embedding_size)\n",
    "source_embedded = embed(source_vector)\n",
    "print()\n",
    "\n",
    "print(\"source_embedded:\", source_embedded.shape)\n",
    "print(source_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66114055-c114-46f3-9613-6d0195e088f6",
   "metadata": {},
   "source": [
    "## ENCODING using embedded SOURCE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf991374-21bb-469a-a19e-bfbc0ef5c7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_output: torch.Size([6, 2, 10])\n",
      "tensor([[[-0.0719,  0.0092,  0.0766,  0.2258, -0.3897,  0.4973,  0.2241,\n",
      "          -0.0364,  0.2452,  0.5391],\n",
      "         [-0.0719,  0.0092,  0.0766,  0.2258, -0.3897,  0.1393, -0.0579,\n",
      "           0.0384,  0.3339,  0.5365]],\n",
      "\n",
      "        [[-0.0766, -0.0131, -0.2374,  0.0349, -0.6761,  0.4118,  0.2044,\n",
      "          -0.0644,  0.1900,  0.5684],\n",
      "         [-0.0207,  0.0344, -0.1456,  0.0855, -0.4137, -0.2120, -0.3217,\n",
      "           0.1128,  0.3892,  0.5949]],\n",
      "\n",
      "        [[ 0.2076,  0.0988, -0.2090,  0.2032, -0.4063,  0.2258, -0.0907,\n",
      "           0.1444,  0.1525,  0.4167],\n",
      "         [-0.6163,  0.0439, -0.1630,  0.2272, -0.0916, -0.4553, -0.6790,\n",
      "           0.2554,  0.4721,  0.5932]],\n",
      "\n",
      "        [[-0.3797,  0.1084, -0.1694,  0.1712, -0.2130,  0.0815, -0.4713,\n",
      "           0.0978,  0.5312,  0.6088],\n",
      "         [-0.0351,  0.2360, -0.6361, -0.0924, -0.0800, -0.4727, -0.1312,\n",
      "           0.2407,  0.5484,  0.5065]],\n",
      "\n",
      "        [[-0.4578,  0.0858, -0.1673,  0.1922, -0.6676,  0.4791,  0.0256,\n",
      "          -0.2110,  0.1974,  0.5321],\n",
      "         [ 0.4759,  0.5119, -0.5321, -0.2327, -0.0092, -0.2722, -0.2529,\n",
      "           0.2939,  0.5546,  0.3741]],\n",
      "\n",
      "        [[-0.5162,  0.0648, -0.2029,  0.2006, -0.7744,  0.3358,  0.0012,\n",
      "          -0.1383,  0.1182,  0.3628],\n",
      "         [-0.2822,  0.4442, -0.3246, -0.0024,  0.0017, -0.1498, -0.5086,\n",
      "           0.0832,  0.4362,  0.3027]]], grad_fn=<CatBackward>)\n",
      "\n",
      "encdoer_hidden directly from rnn: torch.Size([2, 2, 5])\n",
      "tensor([[[-0.5162,  0.0648, -0.2029,  0.2006, -0.7744],\n",
      "         [-0.2822,  0.4442, -0.3246, -0.0024,  0.0017]],\n",
      "\n",
      "        [[ 0.4973,  0.2241, -0.0364,  0.2452,  0.5391],\n",
      "         [ 0.1393, -0.0579,  0.0384,  0.3339,  0.5365]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "encoder_hidden concatenated: torch.Size([1, 2, 10])\n",
      "tensor([[[-0.5162,  0.0648, -0.2029,  0.2006, -0.7744,  0.4973,  0.2241,\n",
      "          -0.0364,  0.2452,  0.5391],\n",
      "         [-0.2822,  0.4442, -0.3246, -0.0024,  0.0017,  0.1393, -0.0579,\n",
      "           0.0384,  0.3339,  0.5365]]], grad_fn=<CatBackward>)\n",
      "\n",
      "encoder_hidden after passing through fc layer: torch.Size([1, 2, 5])\n",
      "tensor([[[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "source_rnn = nn.GRU(embedding_size, hidden_size, bidirectional=True)\n",
    "\n",
    "encoder_output, encoder_hidden = source_rnn(source_embedded)\n",
    "\n",
    "print(\"encoder_output:\", encoder_output.shape)\n",
    "print(encoder_output)\n",
    "print()\n",
    "print(\"encdoer_hidden directly from rnn:\", encoder_hidden.shape)\n",
    "print(encoder_hidden)\n",
    "\n",
    "fc = nn.Linear(hidden_size * 2, hidden_size)\n",
    "encoder_hidden = torch.cat((encoder_hidden[0:1], encoder_hidden[1:2]), -1)\n",
    "print()\n",
    "print(\"encoder_hidden concatenated:\", encoder_hidden.shape)\n",
    "print(encoder_hidden)\n",
    "encoder_hidden = fc(encoder_hidden)\n",
    "print()\n",
    "print(\"encoder_hidden after passing through fc layer:\",encoder_hidden.shape)\n",
    "print(encoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f7df9-492e-4e0e-8be2-d69c1b303047",
   "metadata": {},
   "source": [
    "## The TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f719ecae-614d-45a4-8469-099a04271aee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target vector:\n",
      "tensor([[2, 2],\n",
      "        [1, 5],\n",
      "        [3, 6],\n",
      "        [4, 7],\n",
      "        [0, 8],\n",
      "        [0, 9],\n",
      "        [0, 4]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "target_vector = torch.tensor([[2, 2], [1, 5], [3, 6], [4, 7], [0, 8], [0, 9],[0, 4]])\n",
    "print(\"target vector:\")\n",
    "print(target_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cf016-7c6e-40cf-9f6b-40a11d711937",
   "metadata": {},
   "source": [
    "## DECODING:\n",
    "The decoder's 1st hidden states are the encoder's final hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "752d7d8a-d77c-4ec8-b3d4-f1391b36b86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_embed: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.7465,  1.0051, -0.2568,  0.4765],\n",
      "         [-0.7465,  1.0051, -0.2568,  0.4765]]], grad_fn=<EmbeddingBackward>)\n",
      "\n",
      "h_reshaped (decoder_hidden repeated n times): torch.Size([6, 2, 5])\n",
      "tensor([[[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713]]],\n",
      "       grad_fn=<RepeatBackward>)\n",
      "\n",
      "h_reshaped_encoder_out_cat: torch.Size([6, 2, 15])\n",
      "tensor([[[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616, -0.0719,  0.0092,\n",
      "           0.0766,  0.2258, -0.3897,  0.4973,  0.2241, -0.0364,  0.2452,\n",
      "           0.5391],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713, -0.0719,  0.0092,\n",
      "           0.0766,  0.2258, -0.3897,  0.1393, -0.0579,  0.0384,  0.3339,\n",
      "           0.5365]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616, -0.0766, -0.0131,\n",
      "          -0.2374,  0.0349, -0.6761,  0.4118,  0.2044, -0.0644,  0.1900,\n",
      "           0.5684],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713, -0.0207,  0.0344,\n",
      "          -0.1456,  0.0855, -0.4137, -0.2120, -0.3217,  0.1128,  0.3892,\n",
      "           0.5949]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616,  0.2076,  0.0988,\n",
      "          -0.2090,  0.2032, -0.4063,  0.2258, -0.0907,  0.1444,  0.1525,\n",
      "           0.4167],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713, -0.6163,  0.0439,\n",
      "          -0.1630,  0.2272, -0.0916, -0.4553, -0.6790,  0.2554,  0.4721,\n",
      "           0.5932]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616, -0.3797,  0.1084,\n",
      "          -0.1694,  0.1712, -0.2130,  0.0815, -0.4713,  0.0978,  0.5312,\n",
      "           0.6088],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713, -0.0351,  0.2360,\n",
      "          -0.6361, -0.0924, -0.0800, -0.4727, -0.1312,  0.2407,  0.5484,\n",
      "           0.5065]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616, -0.4578,  0.0858,\n",
      "          -0.1673,  0.1922, -0.6676,  0.4791,  0.0256, -0.2110,  0.1974,\n",
      "           0.5321],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713,  0.4759,  0.5119,\n",
      "          -0.5321, -0.2327, -0.0092, -0.2722, -0.2529,  0.2939,  0.5546,\n",
      "           0.3741]],\n",
      "\n",
      "        [[ 0.1603, -0.0372,  0.0328, -0.3453,  0.0616, -0.5162,  0.0648,\n",
      "          -0.2029,  0.2006, -0.7744,  0.3358,  0.0012, -0.1383,  0.1182,\n",
      "           0.3628],\n",
      "         [-0.0238, -0.2260, -0.2417,  0.0431,  0.0713, -0.2822,  0.4442,\n",
      "          -0.3246, -0.0024,  0.0017, -0.1498, -0.5086,  0.0832,  0.4362,\n",
      "           0.3027]]], grad_fn=<CatBackward>)\n",
      "\n",
      "e: torch.Size([6, 2, 1])\n",
      "tensor([[[0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.1913]],\n",
      "\n",
      "        [[0.1394],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0072],\n",
      "         [0.0000]],\n",
      "\n",
      "        [[0.0000],\n",
      "         [0.0788]]], grad_fn=<ReluBackward0>)\n",
      "\n",
      "attention: torch.Size([6, 2, 1])\n",
      "tensor([[[0.1624],\n",
      "         [0.1589]],\n",
      "\n",
      "        [[0.1624],\n",
      "         [0.1589]],\n",
      "\n",
      "        [[0.1624],\n",
      "         [0.1924]],\n",
      "\n",
      "        [[0.1867],\n",
      "         [0.1589]],\n",
      "\n",
      "        [[0.1636],\n",
      "         [0.1589]],\n",
      "\n",
      "        [[0.1624],\n",
      "         [0.1719]]], grad_fn=<SoftmaxBackward>)\n",
      "\n",
      "context_vector: torch.Size([1, 2, 10])\n",
      "tensor([[[-0.2200,  0.0602, -0.1520,  0.1713, -0.5139,  0.3325, -0.0288,\n",
      "          -0.0316,  0.2461,  0.5072],\n",
      "         [-0.1118,  0.2106, -0.2838,  0.0411, -0.1592, -0.2433, -0.3395,\n",
      "           0.1724,  0.4560,  0.4859]]], grad_fn=<ViewBackward>)\n",
      "\n",
      "rnn_input: torch.Size([1, 2, 14])\n",
      "tensor([[[-0.2200,  0.0602, -0.1520,  0.1713, -0.5139,  0.3325, -0.0288,\n",
      "          -0.0316,  0.2461,  0.5072, -0.7465,  1.0051, -0.2568,  0.4765],\n",
      "         [-0.1118,  0.2106, -0.2838,  0.0411, -0.1592, -0.2433, -0.3395,\n",
      "           0.1724,  0.4560,  0.4859, -0.7465,  1.0051, -0.2568,  0.4765]]],\n",
      "       grad_fn=<CatBackward>)\n",
      "\n",
      "decoder rnn out: torch.Size([1, 2, 5])\n",
      "decoder hidden: torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(10)\n",
    "\n",
    "decoder_hidden = encoder_hidden\n",
    "outputs = torch.zeros(target_seq_len, batch_size, target_vocab_size)\n",
    "\n",
    "embed = nn.Embedding(target_vocab_size, embedding_size)\n",
    "energy = nn.Linear(hidden_size * 3, 1)\n",
    "relu = nn.ReLU()\n",
    "softmax = nn.Softmax(dim = 0)\n",
    "rnn = nn.GRU(hidden_size * 2 + embedding_size, hidden_size)\n",
    "\n",
    "x = target_vector[0]\n",
    "\n",
    "for i in range(1, target_seq_len):\n",
    "    x = x.unsqueeze(0)\n",
    "    x_embed = embed(x)\n",
    "    \n",
    "    print(\"x_embed:\", x_embed.shape)\n",
    "    print(x_embed)\n",
    "    \n",
    "    seq_len = encoder_output.shape[0]\n",
    "    \n",
    "    # Alignment\n",
    "    h_reshaped = decoder_hidden.repeat(seq_len, 1, 1)\n",
    "    print()\n",
    "    print(\"h_reshaped (decoder_hidden repeated n times):\", h_reshaped.shape)\n",
    "    print(h_reshaped)\n",
    "    \n",
    "    h_reshaped_encoder_out_cat = torch.cat((h_reshaped, encoder_output), 2)\n",
    "    print()\n",
    "    print(\"h_reshaped_encoder_out_cat:\", h_reshaped_encoder_out_cat.shape)\n",
    "    print(h_reshaped_encoder_out_cat)\n",
    "    \n",
    "    e = relu(energy(h_reshaped_encoder_out_cat))\n",
    "    print()\n",
    "    print(\"e:\", e.shape)\n",
    "    print(e)\n",
    "    \n",
    "    # Weighing \n",
    "    attention = softmax(e)\n",
    "    print()\n",
    "    print(\"attention:\", attention.shape)\n",
    "    print(attention)\n",
    "    \n",
    "    context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_output)\n",
    "    print()\n",
    "    print(\"context_vector:\", context_vector.shape)\n",
    "    print(context_vector)\n",
    "    \n",
    "    rnn_input = torch.cat((context_vector, x_embed), dim=2)\n",
    "    print()\n",
    "    print(\"rnn_input:\", rnn_input.shape)\n",
    "    print(rnn_input)\n",
    "    \n",
    "    out, decoder_hidden = rnn(rnn_input, decoder_hidden)\n",
    "    print()\n",
    "    print(\"decoder rnn out:\", out.shape)\n",
    "    print(\"decoder hidden:\", decoder_hidden.shape)\n",
    "    \n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
