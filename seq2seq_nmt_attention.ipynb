{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "n0o4ZbCXaAuL"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_regex = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCc5h-_RaB0A",
    "outputId": "9301888c-9e18-4ba7-8c16-7382a4665808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LZv9mQIxaFI3"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    def __init__(self, token_to_idx=None):\n",
    " \n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    " \n",
    "        self._idx_to_token = {idx: token \n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    " \n",
    "    def add_token(self, token):\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    " \n",
    "    def lookup_token(self, token):\n",
    "        return self._token_to_idx[token]\n",
    " \n",
    "    def lookup_index(self, index):\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    " \n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "    \n",
    "class SequenceVocabulary(Vocabulary):\n",
    "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
    "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
    "                 end_seq_token=\"<END>\"):\n",
    " \n",
    "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
    " \n",
    "        self._mask_token = mask_token\n",
    "        self._unk_token = unk_token\n",
    "        self._begin_seq_token = begin_seq_token\n",
    "        self._end_seq_token = end_seq_token\n",
    " \n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = self.add_token(self._unk_token)\n",
    "        self.begin_seq_index = self.add_token(self._begin_seq_token)\n",
    "        self.end_seq_index = self.add_token(self._end_seq_token)\n",
    " \n",
    "    def lookup_token(self, token):\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "        \n",
    "class NMTVectorizer(object):\n",
    "    def __init__(self, source_vocab, target_vocab, max_source_length, max_target_length):\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "        \n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length        \n",
    "        \n",
    "    @classmethod\n",
    "    def from_dataframe(cls, bitext_df):\n",
    "        source_vocab = SequenceVocabulary()\n",
    "        target_vocab = SequenceVocabulary()\n",
    "        \n",
    "        max_source_length = 0\n",
    "        max_target_length = 0\n",
    " \n",
    "        for _, row in bitext_df.iterrows():\n",
    "            source_tokens = punct_regex.tokenize(row[\"source_language\"])\n",
    "            if len(source_tokens) > max_source_length:\n",
    "                max_source_length = len(source_tokens)\n",
    "            for token in source_tokens:\n",
    "                source_vocab.add_token(token)\n",
    "            \n",
    "            target_tokens = punct_regex.tokenize(row[\"target_language\"])\n",
    "            if len(target_tokens) > max_target_length:\n",
    "                max_target_length = len(target_tokens)\n",
    "            for token in target_tokens:\n",
    "                target_vocab.add_token(token)\n",
    "            \n",
    "        return cls(source_vocab, target_vocab, max_source_length, max_target_length)\n",
    "    \n",
    "    def get_vector(self, text, source=True, target=False):\n",
    "        if source:\n",
    "            vocab = self.source_vocab\n",
    "            max_seq_len = self.max_source_length\n",
    "        else:\n",
    "            vocab = self.target_vocab\n",
    "            max_seq_len = self.max_target_length\n",
    "            \n",
    "        vector = np.zeros(max_seq_len + 2, dtype=np.int64)\n",
    "        \n",
    "        vector[0] = vocab.lookup_token(vocab._begin_seq_token)\n",
    "        \n",
    "        for i in range(len(text)):\n",
    "            vector[i+1] = vocab.lookup_token(text[i])\n",
    "        \n",
    "        vector[len(text) + 1] = vocab.lookup_token(vocab._end_seq_token)\n",
    "        \n",
    "        return vector\n",
    "    \n",
    "    def vectorize(self, source_text, target_text):\n",
    "        source_text = punct_regex.tokenize(source_text)\n",
    "        target_text = punct_regex.tokenize(target_text)\n",
    "        \n",
    "        source_vector = self.get_vector(source_text)\n",
    "        target_vector = self.get_vector(target_text, False, True)\n",
    "        \n",
    "        return source_vector, target_vector\n",
    "    \n",
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, text_df, vectorizer):\n",
    "        self.text_df = text_df\n",
    "        self._vectorizer = vectorizer\n",
    " \n",
    "        self.train_df = self.text_df[self.text_df.split=='train']\n",
    "        self.train_size = len(self.train_df)\n",
    " \n",
    "        self.val_df = self.text_df[self.text_df.split=='val']\n",
    "        self.validation_size = len(self.val_df)\n",
    " \n",
    "        self.test_df = self.text_df[self.text_df.split=='test']\n",
    "        self.test_size = len(self.test_df)\n",
    " \n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    " \n",
    "        self.set_split('train')\n",
    " \n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, dataset_csv):\n",
    "        text_df = pd.read_csv(dataset_csv)\n",
    "        train_subset = text_df[text_df.split=='train']\n",
    "        return cls(text_df, NMTVectorizer.from_dataframe(train_subset))\n",
    " \n",
    "    def set_split(self, split=\"train\"):\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "    \n",
    "    def get_vectorizer(self):\n",
    "        return self._vectorizer\n",
    " \n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        row = self._target_df.iloc[index]\n",
    " \n",
    "        source_vector, target_vector = self._vectorizer.vectorize(row.source_language, row.target_language)\n",
    "        \n",
    "        return {\"x_source\": source_vector,\n",
    "               \"y_target\": target_vector}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "LhnmsZOeaIPW"
   },
   "outputs": [],
   "source": [
    "def get_batch_loader(dataset, batch_size):\n",
    "    data_indices = np.arange(dataset._target_size)    \n",
    "    np.random.shuffle(data_indices)\n",
    "    \n",
    "    data = []\n",
    "    for index in data_indices:\n",
    "        data.append(dataset[index])\n",
    "        \n",
    "    data_loader = DataLoader(data, batch_size=batch_size)\n",
    "        \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2Ae1mgSIaLaL"
   },
   "outputs": [],
   "source": [
    "dataset = NMTDataset.load_dataset_and_make_vectorizer(\"../../PyTorchNLPBook/data/nmt/simplest_eng_fra.csv\")\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "padding_index = vectorizer.source_vocab.mask_index\n",
    "\n",
    "EMBEDDING_SIZE = 256\n",
    "RNN_HIDDEN_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "PRINT_EVERY = 10\n",
    "BATCH_SIZE = 64\n",
    "SOURCE_VOCAB_SIZE = len(vectorizer.source_vocab._token_to_idx)\n",
    "TARGET_VOCAB_SIZE = len(vectorizer.target_vocab._token_to_idx)\n",
    "\n",
    "dataset.set_split(\"train\")\n",
    "train_loader = get_batch_loader(dataset, BATCH_SIZE)\n",
    "\n",
    "dataset.set_split(\"val\")\n",
    "val_loader = get_batch_loader(dataset, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QsBaXZ0caTyT"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, bidirectional = True)\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_embed = self.dropout(self.embedding(x))\n",
    "        \n",
    "        encoder_output, hidden = self.rnn(x_embed)\n",
    "        \n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]) , dim = 2))\n",
    "        \n",
    "        return encoder_output, hidden\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(hidden_size * 2 + embedding_size, hidden_size)\n",
    "        \n",
    "        self.energy = nn.Linear(hidden_size * 3, 1)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.softmax = nn.Softmax(dim = 0)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, encoder_states, hidden):\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        x_embed = self.dropout(self.embedding(x))\n",
    "        \n",
    "        seq_len = encoder_states.shape[0]\n",
    "        \n",
    "        h_reshaped = hidden.repeat(seq_len, 1, 1)\n",
    "        \n",
    "        energy = self.relu(self.energy(torch.cat((h_reshaped, encoder_states), dim=2)))\n",
    "        \n",
    "        attention = self.softmax(energy)\n",
    "        \n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention, encoder_states)\n",
    "\n",
    "        rnn_input = torch.cat((context_vector, x_embed), dim=2)\n",
    "\n",
    "        outputs, hidden = self.rnn(rnn_input, hidden)\n",
    "\n",
    "        predictions = self.fc(outputs).squeeze(0)\n",
    "\n",
    "        return predictions, hidden\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, source_vocab_size, \n",
    "                target_vocab_size,\n",
    "                target_output_size,\n",
    "                embedding_size,\n",
    "                rnn_hidden_size,\n",
    "                dropout_p):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab_size,\n",
    "                               embedding_size,\n",
    "                               rnn_hidden_size, \n",
    "                               dropout_p).to(device)\n",
    "        \n",
    "        self.decoder = Decoder(target_vocab_size,\n",
    "                               embedding_size, \n",
    "                               rnn_hidden_size,\n",
    "                               target_output_size, \n",
    "                               dropout_p).to(device)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        encoder_states, hidden = self.encoder(source)\n",
    "        \n",
    "        target_len = target.shape[0]\n",
    "        batch_size = target.shape[1]\n",
    "        target_vocab_len = TARGET_VOCAB_SIZE\n",
    "        \n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_len).to(device)\n",
    "        x = target[0]\n",
    "        \n",
    "        for i in range(1, target_len):\n",
    "            out, hidden = self.decoder(x, encoder_states, hidden)\n",
    "            \n",
    "            outputs[i] = out\n",
    "            \n",
    "            best_guess = out.argmax(1)\n",
    "            \n",
    "            if random.random() < 0.5:\n",
    "                x = target[i]\n",
    "            else:\n",
    "                x = best_guess\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "# model = Seq2Seq(SOURCE_VOCAB_SIZE, \n",
    "#                 TARGET_VOCAB_SIZE, \n",
    "#                 TARGET_VOCAB_SIZE, \n",
    "#                 EMBEDDING_SIZE, \n",
    "#                 RNN_HIDDEN_SIZE,\n",
    "#                 0.1).to(device)\n",
    "# print(vectorizer.max_source_length)\n",
    "# print(vectorizer.max_target_length)\n",
    "# with torch.no_grad():\n",
    "#     for _, batch in enumerate(train_loader):\n",
    "#         print(batch['x_source'].shape)\n",
    "#         print(batch['y_target'].shape)\n",
    "#         source = batch['x_source'].permute(1, 0)\n",
    "#         target = batch['y_target'].permute(1, 0)\n",
    "        \n",
    "#         print(source.shape)\n",
    "#         print(source)\n",
    "        \n",
    "#         print(source[:,0])\n",
    "#         y_pred = model(source.to(device), target.to(device))\n",
    "                \n",
    "#         y_pred = y_pred[1:].reshape(-1, y_pred.shape[2])\n",
    "#         # y_target = target[1:].reshape(-1)\n",
    "        \n",
    "        # loss = loss_func(y_pred.to(device), y_target.to(device))\n",
    "        \n",
    "        # print(\"loss:\", loss)\n",
    "#         break\n",
    "\n",
    "#         print(target.shape)\n",
    "\n",
    "# print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bgJyMWgVaaR2"
   },
   "outputs": [],
   "source": [
    "model = Seq2Seq(SOURCE_VOCAB_SIZE, \n",
    "                TARGET_VOCAB_SIZE, \n",
    "                TARGET_VOCAB_SIZE, \n",
    "                EMBEDDING_SIZE, \n",
    "                RNN_HIDDEN_SIZE,\n",
    "                0.1).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_func = nn.CrossEntropyLoss(ignore_index=padding_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257,
     "referenced_widgets": [
      "e8fbba4e2bec4914861b5be91cc48ed6",
      "c43908db615d4e8686f8a7e5b5ce6ce0",
      "8940a736443a4105814b206d66984a5f",
      "53c8c95233b7433fac6b0a661881d2b9",
      "21214207a87f4d14998bd192fda3c116",
      "cdfd8c687ac14a2099a631a50fa45a2e",
      "a2bc696d44ff4342bf2044e20c1a3136",
      "e09e38e05ab04317a28b814412b39915"
     ]
    },
    "id": "r-OFQcxgabir",
    "outputId": "430733df-fb65-4f73-b79c-19a152d4a183"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e882dca7d0d34b5c94d7cd0e86d88f7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 / 100. Train Loss: 5.160566490013282. Validation Loss: 4.343281284455331\n",
      "Epoch: 10 / 100. Train Loss: 2.3812982644234517. Validation Loss: 3.079926052401143\n",
      "Epoch: 20 / 100. Train Loss: 1.4448584484887295. Validation Loss: 3.0030476047146712\n",
      "Epoch: 30 / 100. Train Loss: 1.0277860352209398. Validation Loss: 3.0656183073597574\n",
      "Epoch: 40 / 100. Train Loss: 0.8199761922542864. Validation Loss: 3.227133081805321\n",
      "Epoch: 50 / 100. Train Loss: 0.6647504586856685. Validation Loss: 3.307442818918536\n",
      "Epoch: 60 / 100. Train Loss: 0.5624445570515587. Validation Loss: 3.407812718422182\n",
      "Epoch: 70 / 100. Train Loss: 0.4639524603848691. Validation Loss: 3.4625304898908067\n",
      "Epoch: 80 / 100. Train Loss: 0.43850858353234673. Validation Loss: 3.498381322430026\n",
      "Epoch: 90 / 100. Train Loss: 0.3841516832996914. Validation Loss: 3.6148998814244426\n",
      "Epoch: 100 / 100. Train Loss: 0.3451737315504702. Validation Loss: 3.713799230514035\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(NUM_EPOCHS), position = 0, leave=True):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_index, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        source = batch['x_source'].permute(1, 0)\n",
    "        target = batch['y_target'].permute(1, 0)\n",
    "\n",
    "        y_pred = model(source.to(device), target.to(device))\n",
    "        y_pred = y_pred[1:].reshape(-1, y_pred.shape[2])\n",
    "        trg = target[1:].reshape(-1)\n",
    "        \n",
    "        loss = loss_func(y_pred.to(device), trg.to(device))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "        \n",
    "    val_running_loss = 0.0\n",
    "    model.eval()\n",
    "        \n",
    "    for batch_index, batch in enumerate(val_loader):\n",
    "        \n",
    "        source = batch['x_source'].permute(1, 0)\n",
    "        target = batch['y_target'].permute(1, 0)\n",
    "\n",
    "        y_pred = model(source.to(device), target.to(device))\n",
    "        y_pred = y_pred[1:].reshape(-1, y_pred.shape[2])\n",
    "        trg = target[1:].reshape(-1)\n",
    "        \n",
    "        loss = loss_func(y_pred.to(device), trg.to(device))\n",
    "\n",
    "        val_running_loss += (loss.item() - val_running_loss) / (batch_index + 1)\n",
    "        \n",
    "    if epoch == 0 or (epoch+1) % PRINT_EVERY == 0:\n",
    "        print(f\"Epoch: {epoch + 1} / {NUM_EPOCHS}. Train Loss: {running_loss}. Validation Loss: {val_running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: you are to do as i tell you\n",
      "Target: vous devez faire comme je vous dis\n",
      "Prediction: vous devez faire comme je te dis\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "\n",
    "def translate_sentence(source_sentence, target_sentence, classifier, vectorizer, decision_threshold=0.5):\n",
    "    source_sentence = preprocess_text(source_sentence)\n",
    "    target_sentence = preprocess_text(target_sentence)\n",
    "    \n",
    "    source_vector, target_vector = vectorizer.vectorize(source_sentence, target_sentence)\n",
    "\n",
    "    source_tensor = torch.tensor(source_vector).unsqueeze(0).permute(1, 0)\n",
    "    target_tensor = torch.tensor(target_vector).unsqueeze(0).permute(1, 0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        result = classifier(source_tensor, target_tensor)\n",
    "        result = result[1:].reshape(-1, result.shape[2])\n",
    "\n",
    "    indices = torch.argmax(result, 1)\n",
    "    word_arr = []\n",
    "    for i in range(indices.shape[0]):\n",
    "        index = indices[i].item()\n",
    "        if index > 3:\n",
    "            word_arr.append(vectorizer.target_vocab._idx_to_token[index])\n",
    "\n",
    "    return ' '.join(word_arr)\n",
    "\n",
    "source_sentence = \"you are to do as i tell you\"\n",
    "target_sentence = \"vous devez faire comme je vous dis\"\n",
    "\n",
    "model = model.cpu()\n",
    "prediction = translate_sentence(source_sentence, target_sentence, model, dataset.get_vectorizer())\n",
    "\n",
    "print(\"Source:\", source_sentence)\n",
    "print(\"Target:\", target_sentence)\n",
    "print(\"Prediction:\", prediction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seq2seq_nmt_attention",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21214207a87f4d14998bd192fda3c116": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "53c8c95233b7433fac6b0a661881d2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e09e38e05ab04317a28b814412b39915",
      "placeholder": "​",
      "style": "IPY_MODEL_a2bc696d44ff4342bf2044e20c1a3136",
      "value": " 50/50 [07:07&lt;00:00,  8.55s/it]"
     }
    },
    "8940a736443a4105814b206d66984a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdfd8c687ac14a2099a631a50fa45a2e",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_21214207a87f4d14998bd192fda3c116",
      "value": 50
     }
    },
    "a2bc696d44ff4342bf2044e20c1a3136": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c43908db615d4e8686f8a7e5b5ce6ce0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cdfd8c687ac14a2099a631a50fa45a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e09e38e05ab04317a28b814412b39915": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8fbba4e2bec4914861b5be91cc48ed6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8940a736443a4105814b206d66984a5f",
       "IPY_MODEL_53c8c95233b7433fac6b0a661881d2b9"
      ],
      "layout": "IPY_MODEL_c43908db615d4e8686f8a7e5b5ce6ce0"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
