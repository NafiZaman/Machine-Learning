{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook shows the inner workings of how pytorch's LSTM module works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some equations that will come in handy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../pics/lstm_equs.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inpt: tensor([[[1., 2., 3.]],\n",
      "\n",
      "        [[4., 5., 6.]]]), inpt shape: torch.Size([2, 1, 3])\n",
      "outs shape: torch.Size([2, 1, 5]) ht shape: torch.Size([1, 1, 5]) ct shape: torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "n_in , n_out = 3, 5\n",
    "\n",
    "inpt = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], dtype = torch.float32)\n",
    "# view input as (seq_len, batch_size, input_size)\n",
    "# 2 words per sentence, 1 sentence per batch, (3 values per word?)\n",
    "inpt = inpt.view(2, 1, 3)\n",
    "print(f\"inpt: {inpt}, inpt shape: {inpt.shape}\")\n",
    "\n",
    "lstm = nn.LSTM(n_in, n_out)\n",
    "\n",
    "outs, (ht, ct) = lstm(inpt)\n",
    "\n",
    "print(f'outs shape: {outs.shape} ht shape: {ht.shape} ct shape: {ct.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2891,  0.2905,  0.2715,  0.3966, -0.2507],\n",
      "        [-0.0736, -0.0087,  0.0653, -0.3394, -0.3174],\n",
      "        [ 0.2433, -0.1049,  0.2185,  0.0255,  0.1468],\n",
      "        [ 0.0983,  0.1626,  0.2217, -0.4142,  0.2251],\n",
      "        [-0.3144, -0.3374,  0.0272, -0.0762,  0.2627]],\n",
      "       grad_fn=<SliceBackward>) torch.Size([5, 5])\n"
     ]
    }
   ],
   "source": [
    "# for name, params in lstm.named_parameters():\n",
    "#     print(name)\n",
    "#     print(params)\n",
    "#     print(params.shape)\n",
    "    \n",
    "# print(lstm.weight_ih_l0)\n",
    "# print(lstm.weight_hh_l0)\n",
    "# print(lstm.bias_ih_l0,lstm.bias_ih_l0.shape)\n",
    "# print(lstm.bias_hh_l0, lstm.bias_hh_l0.shape)\n",
    "\n",
    "w_ih_l0 = lstm.weight_ih_l0\n",
    "wii = w_ih_l0[0*n_out : 1*n_out, :]\n",
    "wif = w_ih_l0[1*n_out : 2*n_out, :]\n",
    "wig = w_ih_l0[2*n_out : 3*n_out, :]\n",
    "wio = w_ih_l0[3*n_out : 4*n_out, :]\n",
    "\n",
    "w_hh_l0 = lstm.weight_hh_l0\n",
    "whi = w_hh_l0[0*n_out : 1*n_out, :]\n",
    "whf = w_hh_l0[1*n_out : 2*n_out, :]\n",
    "whg = w_hh_l0[2*n_out : 3*n_out, :]\n",
    "who = w_hh_l0[3*n_out : 4*n_out, :]\n",
    "\n",
    "b_ih_l0 = lstm.bias_ih_l0\n",
    "bii = b_ih_l0[0*n_out : 1*n_out]\n",
    "bif = b_ih_l0[1*n_out : 2*n_out]\n",
    "big = b_ih_l0[2*n_out : 3*n_out]\n",
    "bio = b_ih_l0[3*n_out : 4*n_out]\n",
    "\n",
    "b_hh_l0 = lstm.bias_hh_l0\n",
    "bhi = b_hh_l0[0*n_out : 1*n_out]\n",
    "bhf = b_hh_l0[1*n_out : 2*n_out]\n",
    "bhg = b_hh_l0[2*n_out : 3*n_out]\n",
    "bho = b_hh_l0[3*n_out : 4*n_out]\n",
    "\n",
    "print(whi, whi.shape)\n",
    "# print(bhf, bhf.shape)\n",
    "# print(bhg, bhg.shape)\n",
    "# print(bho, bho.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = tensor([[[1., 2., 3.]],\n",
      "\n",
      "        [[4., 5., 6.]]]), input shape: torch.Size([2, 1, 3])\n",
      "hid = tensor([[0., 0., 0., 0., 0.]]), hid shape: torch.Size([1, 5])\n",
      "c_0 = tensor([[0., 0., 0., 0., 0.]]), c_0 shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([1, 2, 3, 4, 5, 6], dtype=torch.float32)\n",
    "input = input.view(2, 1, 3)\n",
    "\n",
    "hid = torch.zeros((1, 5), dtype = torch.float32)\n",
    "c_0 = torch.zeros((1, 5), dtype = torch.float32)\n",
    "\n",
    "print(f'input = {input}, input shape: {input.shape}')\n",
    "print(f'hid = {hid}, hid shape: {hid.shape}')\n",
    "print(f'c_0 = {c_0}, c_0 shape: {c_0.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "output = []\n",
    "for i in range(input.shape[0]):\n",
    "    x = input[i]\n",
    "#     print(x)\n",
    "\n",
    "    # calculation of i\n",
    "    i1 = x @ torch.transpose(wii, 0, 1) + bii\n",
    "    i2 = hid @ torch.transpose(whi, 0, 1) + bhi\n",
    "    i = sigmoid(i1+i2)\n",
    "    \n",
    "    # calculation of f\n",
    "    f1 = x @ torch.transpose(wif, 0, 1) + bif\n",
    "    f2 = hid @ torch.transpose(whf, 0, 1) + bhf\n",
    "    f = sigmoid(f1 + f2)\n",
    "    \n",
    "    # calculation of g\n",
    "    g1 = x @ torch.transpose(wig, 0, 1) + big\n",
    "    g2 = hid @ torch.transpose(whg, 0, 1) + bhg\n",
    "    g = tanh(g1 + g2)\n",
    "    \n",
    "    # calculation of 0\n",
    "    o1 = x @ torch.transpose(wio, 0, 1) + bio\n",
    "    o2 = hid @ torch.transpose(who, 0, 1) + bho\n",
    "    o = sigmoid(o1 + o2)\n",
    "    \n",
    "    c_prime = (f * c_0) + (i * g)\n",
    "    h_prime = o * tanh(c_prime)\n",
    "    \n",
    "    hid = h_prime\n",
    "    c_0 = c_prime\n",
    "    \n",
    "    output.append(hid)\n",
    "\n",
    "hid = hid.unsqueeze(0)\n",
    "c_0 = c_0.unsqueeze(0)\n",
    "output = torch.stack(output, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch results: \n",
      "tensor([[[ 0.1558, -0.1345,  0.1861,  0.1100,  0.0798]],\n",
      "\n",
      "        [[ 0.2747, -0.0403,  0.3198,  0.3457,  0.1773]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "tensor([[[ 0.2747, -0.0403,  0.3198,  0.3457,  0.1773]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "tensor([[[ 0.4520, -0.9810,  0.7305,  0.3731,  0.2464]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "My results: \n",
      "tensor([[[ 0.1558, -0.1345,  0.1861,  0.1100,  0.0798]],\n",
      "\n",
      "        [[ 0.2747, -0.0403,  0.3198,  0.3457,  0.1773]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "tensor([[[ 0.2747, -0.0403,  0.3198,  0.3457,  0.1773]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "tensor([[[ 0.4520, -0.9810,  0.7305,  0.3731,  0.2464]]],\n",
      "       grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "outs shape: torch.Size([2, 1, 5]) ht shape: torch.Size([1, 1, 5]) ct shape: torch.Size([1, 1, 5])\n",
      "output shape: torch.Size([2, 1, 5]) hid shape: torch.Size([1, 1, 5]) c_0 shape: torch.Size([1, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch results: \")\n",
    "print(outs)\n",
    "print()\n",
    "print(ht)\n",
    "print()\n",
    "print(ct)\n",
    "\n",
    "print()\n",
    "print(\"My results: \")\n",
    "print(output)\n",
    "print()\n",
    "print(hid)\n",
    "print()\n",
    "print(c_0)\n",
    "\n",
    "print()\n",
    "print(f'outs shape: {outs.shape} ht shape: {ht.shape} ct shape: {ct.shape}')\n",
    "print(f'output shape: {output.shape} hid shape: {hid.shape} c_0 shape: {c_0.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch LSTM with batch_first = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "input_dim = 5\n",
    "hidden_dim = 10\n",
    "n_layers = 1\n",
    "\n",
    "lstm_layer = nn.LSTM(input_dim, hidden_dim, \n",
    "                     n_layers, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "seq_len = 1\n",
    "\n",
    "inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "hidden_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "cell_state = torch.randn(n_layers, batch_size, hidden_dim)\n",
    "hidden = (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([1, 1, 10])\n",
      "Hidden:  (tensor([[[-0.0853, -0.3509, -0.1043,  0.1794,  0.1093,  0.2788, -0.1967,\n",
      "          -0.3151, -0.1653, -0.0048]]], grad_fn=<StackBackward>), tensor([[[-0.1763, -1.0270, -0.3605,  0.2844,  0.1600,  0.5662, -0.5406,\n",
      "          -0.4369, -0.2489, -0.0196]]], grad_fn=<StackBackward>))\n"
     ]
    }
   ],
   "source": [
    "out, hidden = lstm_layer(inp, hidden)\n",
    "print(\"Output shape: \", out.shape)\n",
    "print(\"Hidden: \", hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len = 3\n",
    "# inp = torch.randn(batch_size, seq_len, input_dim)\n",
    "# out, hidden = lstm_layer(inp, hidden)\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, params in lstm_layer.named_parameters():\n",
    "#     print(name, params)\n",
    "#     break\n",
    "\n",
    "w_ih_l0 = lstm_layer.weight_ih_l0\n",
    "wii = w_ih_l0[0*hidden_dim : 1*hidden_dim, :]\n",
    "wif = w_ih_l0[1*hidden_dim : 2*hidden_dim, :]\n",
    "wig = w_ih_l0[2*hidden_dim : 3*hidden_dim, :]\n",
    "wio = w_ih_l0[3*hidden_dim : 4*hidden_dim, :]\n",
    "\n",
    "w_hh_l0 = lstm_layer.weight_hh_l0\n",
    "whi = w_hh_l0[0*hidden_dim : 1*hidden_dim, :]\n",
    "whf = w_hh_l0[1*hidden_dim : 2*hidden_dim, :]\n",
    "whg = w_hh_l0[2*hidden_dim : 3*hidden_dim, :]\n",
    "who = w_hh_l0[3*hidden_dim : 4*hidden_dim, :]\n",
    "\n",
    "b_ih_l0 = lstm_layer.bias_ih_l0\n",
    "bii = b_ih_l0[0*hidden_dim : 1*hidden_dim]\n",
    "bif = b_ih_l0[1*hidden_dim : 2*hidden_dim]\n",
    "big = b_ih_l0[2*hidden_dim : 3*hidden_dim]\n",
    "bio = b_ih_l0[3*hidden_dim : 4*hidden_dim]\n",
    "\n",
    "b_hh_l0 = lstm_layer.bias_hh_l0\n",
    "bhi = b_hh_l0[0*hidden_dim : 1*hidden_dim]\n",
    "bhf = b_hh_l0[1*hidden_dim : 2*hidden_dim]\n",
    "bhg = b_hh_l0[2*hidden_dim : 3*hidden_dim]\n",
    "bho = b_hh_l0[3*hidden_dim : 4*hidden_dim]\n",
    "\n",
    "# print(wii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2624, -0.6198, -0.7153,  0.0834,  0.2980]]]) torch.Size([1, 1, 5])\n",
      "tensor([[ 2.0028,  0.5610, -1.6287, -1.3715, -1.1648, -1.2502,  0.4156,  0.7394,\n",
      "         -0.8678,  0.5870]]) torch.Size([1, 10])\n",
      "tensor([[-0.1618, -1.3426,  0.8099,  1.0417,  0.4967,  1.7153, -1.1099,  0.3573,\n",
      "         -0.3369, -0.1951]]) torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "input = inp\n",
    "hid = hidden_state\n",
    "c_0 = cell_state\n",
    "\n",
    "print(input, input.shape)\n",
    "# print(hid, hid.shape)\n",
    "# print(c_0, c_0.shape)\n",
    "\n",
    "hid = hid.squeeze(0)\n",
    "c_0 = c_0.squeeze(0)\n",
    "\n",
    "print(hid, hid.shape)\n",
    "print(c_0, c_0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "output = []\n",
    "\n",
    "for i in range(input.shape[0]):\n",
    "    x = input[i]\n",
    "#     print(x)\n",
    "\n",
    "    # calculation of i\n",
    "    i1 = x @ torch.transpose(wii, 0, 1) + bii\n",
    "    i2 = hid @ torch.transpose(whi, 0, 1) + bhi\n",
    "    i = sigmoid(i1+i2)\n",
    "    \n",
    "    # calculation of f\n",
    "    f1 = x @ torch.transpose(wif, 0, 1) + bif\n",
    "    f2 = hid @ torch.transpose(whf, 0, 1) + bhf\n",
    "    f = sigmoid(f1 + f2)\n",
    "    \n",
    "    # calculation of g\n",
    "    g1 = x @ torch.transpose(wig, 0, 1) + big\n",
    "    g2 = hid @ torch.transpose(whg, 0, 1) + bhg\n",
    "    g = tanh(g1 + g2)\n",
    "    \n",
    "    # calculation of 0\n",
    "    o1 = x @ torch.transpose(wio, 0, 1) + bio\n",
    "    o2 = hid @ torch.transpose(who, 0, 1) + bho\n",
    "    o = sigmoid(o1 + o2)\n",
    "    \n",
    "    c_prime = (f * c_0) + (i * g)\n",
    "    h_prime = o * tanh(c_prime)\n",
    "    \n",
    "    hid = h_prime\n",
    "    c_0 = c_prime\n",
    "    \n",
    "    output.append(hid)\n",
    "\n",
    "hid = hid.unsqueeze(0)\n",
    "c_0 = c_0.unsqueeze(0)\n",
    "output = torch.stack(output, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch results: \n",
      "tensor([[[-0.0853, -0.3509, -0.1043,  0.1794,  0.1093,  0.2788, -0.1967,\n",
      "          -0.3151, -0.1653, -0.0048]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "tensor([[[-0.0853, -0.3509, -0.1043,  0.1794,  0.1093,  0.2788, -0.1967,\n",
      "          -0.3151, -0.1653, -0.0048]]], grad_fn=<StackBackward>)\n",
      "\n",
      "tensor([[[-0.1763, -1.0270, -0.3605,  0.2844,  0.1600,  0.5662, -0.5406,\n",
      "          -0.4369, -0.2489, -0.0196]]], grad_fn=<StackBackward>)\n",
      "\n",
      "My results: \n",
      "tensor([[[-0.0853, -0.3509, -0.1043,  0.1794,  0.1093,  0.2788, -0.1967,\n",
      "          -0.3151, -0.1653, -0.0048]]], grad_fn=<StackBackward>)\n",
      "\n",
      "tensor([[[-0.0853, -0.3509, -0.1043,  0.1794,  0.1093,  0.2788, -0.1967,\n",
      "          -0.3151, -0.1653, -0.0048]]], grad_fn=<UnsqueezeBackward0>)\n",
      "\n",
      "tensor([[[-0.1763, -1.0270, -0.3605,  0.2844,  0.1600,  0.5662, -0.5406,\n",
      "          -0.4369, -0.2489, -0.0196]]], grad_fn=<UnsqueezeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Pytorch results: \")\n",
    "print(out)\n",
    "print()\n",
    "print(hidden[0])\n",
    "print()\n",
    "print(hidden[1])\n",
    "\n",
    "print()\n",
    "print(\"My results: \")\n",
    "print(output)\n",
    "print()\n",
    "print(hid)\n",
    "print()\n",
    "print(c_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
