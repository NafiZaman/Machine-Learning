{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_encodings(embeds, input):\n",
    "    pe = torch.zeros((embeds.shape[0], embeds.shape[1], embeds.shape[2]), dtype = torch.float)\n",
    "\n",
    "    for pos in range(input.shape[-1]):\n",
    "        v = pe[0, pos]\n",
    "        for i in range(v.shape[0]):\n",
    "            if i % 2 ==0:\n",
    "                pwr = (2 * i) / dim_in\n",
    "                v[i] = np.sin(pos / (1e4 ** pwr))\n",
    "            else:\n",
    "                pwr = (2 * i) / dim_in\n",
    "                v[i] = np.cos(pos / (1e4 ** pwr))\n",
    "\n",
    "        pe[0, pos] = v\n",
    "\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in, attn_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.wq = nn.Linear(dim_in, attn_dim)\n",
    "        self.wk = nn.Linear(dim_in, attn_dim)\n",
    "        self.wv = nn.Linear(dim_in, attn_dim)\n",
    "        \n",
    "    def forward(self, input, q, k, v, mask_inputs):\n",
    "\n",
    "        if input == None:\n",
    "            query = q\n",
    "            key = k\n",
    "            value = v\n",
    "        else:\n",
    "            query = self.wq(input)\n",
    "            key = self.wk(input)\n",
    "            value = self.wv(input)\n",
    "        \n",
    "        score = torch.bmm(query, torch.transpose(key, 1, 2))\n",
    "        scale = query.shape[-1] ** 0.5\n",
    "        score = score / scale\n",
    "        \n",
    "        if mask_inputs:\n",
    "            masked_attention = torch.zeros(score.shape[0], score.shape[1], score.shape[2])\n",
    "            \n",
    "            for i in range(masked_attention.shape[1]):\n",
    "                masked_attention[0, i, (i+1):] = -float('inf')\n",
    "\n",
    "            score += masked_attention\n",
    "            \n",
    "        softmax = F.softmax(score, dim = -1)\n",
    "        return torch.bmm(softmax, value)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_in, attn_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, attn_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(num_heads*attn_dim, dim_in)\n",
    "        \n",
    "    def forward(self, input, q, k, v, mask_inputs):\n",
    "        \n",
    "        if input == None:\n",
    "            lin = nn.Linear(q.shape[-1] * len(self.heads), q.shape[-1])\n",
    "            return lin(\n",
    "                torch.cat([h(input, q, k, v, mask_inputs) for h in self.heads], dim = -1)\n",
    "            )\n",
    "        \n",
    "        else:\n",
    "            return self.linear(\n",
    "                torch.cat([h(input, q, k, v, mask_inputs) for h in self.heads], dim = -1)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risidual & Feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Risidual(nn.Module):\n",
    "    def __init__(self, dropout, dim_in):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = nn.LayerNorm(dim_in)\n",
    "        \n",
    "    def forward(self, prev_layer_input, prev_layer_output):\n",
    "        z = prev_layer_input + prev_layer_output\n",
    "        return self.norm(z[-1] + self.dropout(prev_layer_output))\n",
    "    \n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim_in, dim_feedforward):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(dim_in, dim_feedforward)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin2 = nn.Linear(dim_feedforward, dim_in)\n",
    "    \n",
    "    def forward(self, risidual_output):\n",
    "        encoder_output = self.lin1(risidual_output)\n",
    "        encoder_output = self.relu(encoder_output)\n",
    "        encoder_output = self.lin2(encoder_output)\n",
    "        \n",
    "        return encoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                dim_in,\n",
    "                attn_dim,\n",
    "                num_heads,\n",
    "                dim_feedforward,\n",
    "                dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads,dim_in,attn_dim)\n",
    "        \n",
    "        self.risidual_mha = Risidual(dropout,dim_in)\n",
    "        \n",
    "        self.feed_forward = FeedForward(dim_in, dim_feedforward)\n",
    "        \n",
    "        self.risidual_ff = Risidual(dropout,dim_in)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        mha = self.mha(input, None, None, None, False)\n",
    "        risidual_mha = self.risidual_mha(input, mha)\n",
    "        feed_forward = self.feed_forward(risidual_mha)\n",
    "        risidual_ff = self.risidual_ff(risidual_mha, feed_forward)\n",
    "        \n",
    "        return risidual_ff\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers,\n",
    "                 dim_in,\n",
    "                 seq_len,\n",
    "                 attn_dim,\n",
    "                 num_heads,\n",
    "                 vocab_size,\n",
    "                 dim_feedforward,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "    \n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerEncoderLayer(dim_in,\n",
    "                                    attn_dim,\n",
    "                                    num_heads,\n",
    "                                    dim_feedforward,\n",
    "                                    dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, dim_in)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        embeds = self.embedding_layer(input)\n",
    "        embeds += get_positional_encodings(embeds, input)\n",
    "        \n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            embeds = encoder_layer(embeds)\n",
    "            \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, dim_in, attn_dim, num_heads, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.mha = MultiHeadAttention(num_heads,dim_in,attn_dim)\n",
    "        self.risidual_mha = Risidual(dropout, dim_in)\n",
    "        self.risidual_mha2 = Risidual(dropout, dim_in)\n",
    "        self.feed_forward = FeedForward(dim_in,dim_feedforward)\n",
    "        self.risidual_ff = Risidual(dropout, dim_in)\n",
    "        \n",
    "    def forward(self, input, encoder_output):\n",
    "        mha = self.mha(input, None, None, None, True)\n",
    "        risidual_mha = self.risidual_mha(input, mha) # this is the query vector for the next MHAL\n",
    "        mha2 = self.mha(None, risidual_mha, encoder_output, encoder_output, False)\n",
    "        risidual_mha2 = self.risidual_mha2(risidual_mha, mha2)\n",
    "        feed_forward = self.feed_forward(risidual_mha2)\n",
    "        risidual_ff = self.risidual_ff(risidual_mha2, feed_forward)\n",
    "        \n",
    "        return risidual_ff\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers, \n",
    "                 dim_in, \n",
    "                 seq_len, \n",
    "                 attn_dim, \n",
    "                 num_heads, \n",
    "                 vocab_size, \n",
    "                 dim_feedforward,\n",
    "                 dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            TransformerDecoderLayer(dim_in,\n",
    "                                    attn_dim,\n",
    "                                    num_heads, \n",
    "                                    dim_feedforward, \n",
    "                                    dropout)\n",
    "            \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.embedding_layer = nn.Embedding(vocab_size, dim_in)\n",
    "        \n",
    "    def forward(self, input, encoder_output):\n",
    "        embeds = self.embedding_layer(input)\n",
    "        embeds += get_positional_encodings(embeds, input)\n",
    "        \n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            embeds = decoder_layer(embeds, encoder_output)\n",
    "            \n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(1)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                dim_in,\n",
    "                seq_len,\n",
    "                attn_dim,\n",
    "                num_heads,\n",
    "                vocab_size,\n",
    "                dim_feedforward,\n",
    "                dropout,\n",
    "                num_layers,\n",
    "                target_seq_len,\n",
    "                target_vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = TransformerEncoder(num_layers,\n",
    "                                         dim_in,\n",
    "                                         seq_len,\n",
    "                                         attn_dim,\n",
    "                                         num_heads,\n",
    "                                         vocab_size,\n",
    "                                         dim_feedforward,\n",
    "                                         dropout)\n",
    "        \n",
    "        self.decoder = TransformerDecoder(num_layers,\n",
    "                                         dim_in,\n",
    "                                         target_seq_len,\n",
    "                                         attn_dim,\n",
    "                                         num_heads,\n",
    "                                         target_vocab_size,\n",
    "                                         dim_feedforward,\n",
    "                                         dropout)\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        \n",
    "    def forward(self, src, trgt):\n",
    "        encoder_output = self.encoder(src)\n",
    "        decoder_output = self.decoder(trgt, encoder_output)\n",
    "        \n",
    "        decoder_output = decoder_output.view(1, -1)\n",
    "        lin = nn.Linear(decoder_output.shape[-1], self.target_vocab_size)\n",
    "        output = F.softmax(lin(decoder_output), dim = -1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1386, 0.0569, 0.0419, 0.0828, 0.0496, 0.0664, 0.2338, 0.0395, 0.0853,\n",
      "         0.0541, 0.0708, 0.0801]], grad_fn=<SoftmaxBackward>)\n",
      "torch.Size([1, 12])\n"
     ]
    }
   ],
   "source": [
    "dim_in = 5\n",
    "seq_len = 4\n",
    "batch_size = 3\n",
    "attn_dim = 64\n",
    "num_heads = 8\n",
    "vocab_size = 10\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "num_layers = 6\n",
    "target_seq_len = 5\n",
    "target_vocab_size = 12\n",
    "\n",
    "src = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "target = torch.randint(0, target_vocab_size, (batch_size, target_seq_len))\n",
    "\n",
    "transformer = Transformer(dim_in,\n",
    "                          seq_len,\n",
    "                          attn_dim,\n",
    "                          num_heads,\n",
    "                          vocab_size,\n",
    "                          dim_feedforward,\n",
    "                          dropout,\n",
    "                          num_layers,\n",
    "                          target_seq_len,\n",
    "                          target_vocab_size)\n",
    "\n",
    "transformer_output = transformer(src, target)\n",
    "print(transformer_output)\n",
    "print(transformer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 4, 7, 2],\n",
      "        [1, 5, 0, 5],\n",
      "        [7, 2, 2, 9]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(src)\n",
    "print(src.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
