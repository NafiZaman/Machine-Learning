{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, 'This': 1, 'is': 2, 'cool': 3, 'Movie': 4, 'sucks': 5}\n",
      "\n",
      "{0: '<PAD>', 1: 'This', 2: 'is', 3: 'cool', 4: 'Movie', 5: 'sucks'}\n",
      "[[1, 2, 3], [4, 5, 0]]\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 0]])\n",
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is cool\", \"Movie sucks\"]\n",
    "\n",
    "word_to_idx = {\"<PAD>\" : 0}\n",
    "idx_to_word = {0 : \"<PAD>\"}\n",
    "\n",
    "for sent in sentences:\n",
    "    for word in sent.split(' '):\n",
    "        if word not in word_to_idx:\n",
    "            idx_to_word[len(word_to_idx)] = word\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "            \n",
    "print(word_to_idx)\n",
    "print()\n",
    "print(idx_to_word)\n",
    "\n",
    "vectors = []\n",
    "\n",
    "for sent in sentences:\n",
    "    word_arr = []\n",
    "    for word in sent.split(' '):\n",
    "        word_arr.append(word_to_idx[word])\n",
    "    if len(sent.split(' ')) < 3:\n",
    "        for i in range(3 - len(sent.split(' '))):\n",
    "            word_arr.append(word_to_idx[\"<PAD>\"])\n",
    "    vectors.append(word_arr)\n",
    "    \n",
    "print(vectors)\n",
    "\n",
    "x = torch.from_numpy(np.asarray(vectors))\n",
    "\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after passing through embedding layer:\n",
      "tensor([[[-0.1002, -0.6092, -0.9798, -1.6091],\n",
      "         [ 0.4391,  1.1712,  1.7674, -0.0954],\n",
      "         [ 0.1394, -1.5785, -0.3206, -0.2993]],\n",
      "\n",
      "        [[-0.7984,  0.3357,  0.2753,  1.7163],\n",
      "         [-0.0561,  0.9107, -1.3924,  2.6891],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]], grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "emb_dim = 4\n",
    "rnn_hidden_size = 5\n",
    "emb = nn.Embedding(len(word_to_idx), emb_dim, padding_idx=0)\n",
    "x_emb = emb(x)\n",
    "\n",
    "print(\"X after passing through embedding layer:\")\n",
    "print(x_emb)\n",
    "print(x_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN weights: \n",
      "Parameter containing:\n",
      "tensor([[ 0.2304, -0.1974, -0.0867,  0.2099],\n",
      "        [-0.4210,  0.2682, -0.0920,  0.2275],\n",
      "        [ 0.0622, -0.0548,  0.1240,  0.0221],\n",
      "        [ 0.1633, -0.1743, -0.0326, -0.0403],\n",
      "        [ 0.0648, -0.0018,  0.3909,  0.1392],\n",
      "        [-0.1665, -0.2701, -0.0750, -0.1929],\n",
      "        [-0.1433,  0.0214,  0.2666,  0.2431],\n",
      "        [-0.4372,  0.2772,  0.1249,  0.4242],\n",
      "        [ 0.2952, -0.4075, -0.4252, -0.2157],\n",
      "        [ 0.3927, -0.0745,  0.1914, -0.2078],\n",
      "        [ 0.4388, -0.1892,  0.3354,  0.0053],\n",
      "        [-0.2356,  0.2299, -0.2374,  0.1315],\n",
      "        [-0.1291, -0.0490, -0.4299, -0.2132],\n",
      "        [ 0.2427, -0.1087,  0.4454,  0.3585],\n",
      "        [-0.0209, -0.2985,  0.2723,  0.1388]], requires_grad=True)\n",
      "\n",
      "Individual weight matrices:\n",
      "tensor([[ 0.2304, -0.1974, -0.0867,  0.2099],\n",
      "        [-0.4210,  0.2682, -0.0920,  0.2275],\n",
      "        [ 0.0622, -0.0548,  0.1240,  0.0221],\n",
      "        [ 0.1633, -0.1743, -0.0326, -0.0403],\n",
      "        [ 0.0648, -0.0018,  0.3909,  0.1392]], grad_fn=<SliceBackward>)\n",
      "\n",
      "tensor([[-0.1665, -0.2701, -0.0750, -0.1929],\n",
      "        [-0.1433,  0.0214,  0.2666,  0.2431],\n",
      "        [-0.4372,  0.2772,  0.1249,  0.4242],\n",
      "        [ 0.2952, -0.4075, -0.4252, -0.2157],\n",
      "        [ 0.3927, -0.0745,  0.1914, -0.2078]], grad_fn=<SliceBackward>)\n",
      "\n",
      "tensor([[ 0.4388, -0.1892,  0.3354,  0.0053],\n",
      "        [-0.2356,  0.2299, -0.2374,  0.1315],\n",
      "        [-0.1291, -0.0490, -0.4299, -0.2132],\n",
      "        [ 0.2427, -0.1087,  0.4454,  0.3585],\n",
      "        [-0.0209, -0.2985,  0.2723,  0.1388]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "rnn = nn.GRU(emb_dim, rnn_hidden_size, batch_first = True,bias=False)\n",
    "print(\"RNN weights: \")\n",
    "print(rnn.weight_ih_l0)\n",
    "        \n",
    "print()\n",
    "print(\"Individual weight matrices:\")\n",
    "wih_r = rnn.weight_ih_l0[0:5, :]\n",
    "wih_z = rnn.weight_ih_l0[5:10,:]\n",
    "wih_n = rnn.weight_ih_l0[10:15, :]\n",
    "print(wih_r)\n",
    "print()\n",
    "print(wih_z)\n",
    "print()\n",
    "print(wih_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN weights:\n",
      "Parameter containing:\n",
      "tensor([[-0.2891,  0.2905,  0.2715,  0.3966, -0.2507],\n",
      "        [-0.0736, -0.0087,  0.0653, -0.3394, -0.3174],\n",
      "        [ 0.2433, -0.1049,  0.2185,  0.0255,  0.1468],\n",
      "        [ 0.0983,  0.1626,  0.2217, -0.4142,  0.2251],\n",
      "        [-0.3144, -0.3374,  0.0272, -0.0762,  0.2627],\n",
      "        [-0.2590, -0.3976,  0.3255, -0.0663,  0.2515],\n",
      "        [ 0.1438, -0.3354,  0.0898,  0.1074, -0.2994],\n",
      "        [-0.2122,  0.1525,  0.0801, -0.1902, -0.1354],\n",
      "        [ 0.4096, -0.0827,  0.2521,  0.1936, -0.2891],\n",
      "        [-0.3803,  0.4293,  0.0234,  0.3065,  0.0927],\n",
      "        [ 0.1438,  0.3341,  0.4241, -0.2968,  0.0559],\n",
      "        [ 0.3337,  0.3240,  0.2778, -0.3237, -0.3220],\n",
      "        [-0.2705,  0.0562,  0.4457, -0.2825,  0.2383],\n",
      "        [-0.2475, -0.4205, -0.0951,  0.2577,  0.4152],\n",
      "        [-0.2777,  0.0971,  0.3859,  0.2963,  0.2787]], requires_grad=True)\n",
      "\n",
      "Individual weight matrices:\n",
      "tensor([[-0.2891,  0.2905,  0.2715,  0.3966, -0.2507],\n",
      "        [-0.0736, -0.0087,  0.0653, -0.3394, -0.3174],\n",
      "        [ 0.2433, -0.1049,  0.2185,  0.0255,  0.1468],\n",
      "        [ 0.0983,  0.1626,  0.2217, -0.4142,  0.2251],\n",
      "        [-0.3144, -0.3374,  0.0272, -0.0762,  0.2627]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "\n",
      "tensor([[-0.2590, -0.3976,  0.3255, -0.0663,  0.2515],\n",
      "        [ 0.1438, -0.3354,  0.0898,  0.1074, -0.2994],\n",
      "        [-0.2122,  0.1525,  0.0801, -0.1902, -0.1354],\n",
      "        [ 0.4096, -0.0827,  0.2521,  0.1936, -0.2891],\n",
      "        [-0.3803,  0.4293,  0.0234,  0.3065,  0.0927]],\n",
      "       grad_fn=<SliceBackward>)\n",
      "\n",
      "tensor([[ 0.1438,  0.3341,  0.4241, -0.2968,  0.0559],\n",
      "        [ 0.3337,  0.3240,  0.2778, -0.3237, -0.3220],\n",
      "        [-0.2705,  0.0562,  0.4457, -0.2825,  0.2383],\n",
      "        [-0.2475, -0.4205, -0.0951,  0.2577,  0.4152],\n",
      "        [-0.2777,  0.0971,  0.3859,  0.2963,  0.2787]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"RNN weights:\")\n",
    "print(rnn.weight_hh_l0)\n",
    "\n",
    "print()\n",
    "print(\"Individual weight matrices:\")\n",
    "whh_r = rnn.weight_hh_l0[0:5, :]\n",
    "whh_z = rnn.weight_hh_l0[5:10,:]\n",
    "whh_n = rnn.weight_hh_l0[10:15, :]\n",
    "print(whh_r)\n",
    "print()\n",
    "print(whh_z)\n",
    "print()\n",
    "print(whh_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Pytorch:\n",
      "tensor([[[-0.0941, -0.0626,  0.4790, -0.2041, -0.1372],\n",
      "         [ 0.2963, -0.1015,  0.0473,  0.3765, -0.0092],\n",
      "         [ 0.2581, -0.2344,  0.1297,  0.2794,  0.1541]],\n",
      "\n",
      "        [[-0.1746,  0.1412, -0.0872,  0.3289,  0.1479],\n",
      "         [-0.4669,  0.3670, -0.0742,  0.2993, -0.0806],\n",
      "         [-0.2444,  0.1372, -0.0368,  0.1408, -0.0089]]],\n",
      "       grad_fn=<TransposeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Result of Pytorch:\")\n",
    "out, hx = rnn(x_emb)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_in = x_emb.permute(1, 0, 2)\n",
    "# print(\"x after permuted:\")\n",
    "# print(x_in)\n",
    "# print()\n",
    "\n",
    "# print(\"My result\")\n",
    "\n",
    "batch_size, seq_size, feat_size = x_emb.shape\n",
    "my_out = []\n",
    "hidden_t = torch.zeros((batch_size, rnn_hidden_size))\n",
    "sigmoid = nn.Sigmoid()\n",
    "tanh = nn.Tanh()\n",
    "\n",
    "for i in range(seq_size):\n",
    "    x_inp = x_in[i]\n",
    "    \n",
    "    r1 = x_inp @ torch.transpose(wih_r, 0, 1)\n",
    "    r2 = hidden_t @ torch.transpose(whh_r, 0, 1)\n",
    "    \n",
    "    r = sigmoid(r1 + r2)\n",
    "    \n",
    "    z1 = x_inp @ torch.transpose(wih_z, 0, 1)\n",
    "    z2 = hidden_t @ torch.transpose(whh_z, 0, 1)\n",
    "    \n",
    "    z = sigmoid(z1 + z2)\n",
    "    \n",
    "    n1 = x_inp @ torch.transpose(wih_n, 0, 1)\n",
    "    n2 = r * (hidden_t @ torch.transpose(whh_n, 0, 1))\n",
    "    \n",
    "    n = tanh(n1+n2)\n",
    "    \n",
    "    hidden_t = (1 - z) * n + (z * hidden_t)\n",
    "    \n",
    "    my_out.append(hidden_t)\n",
    "    \n",
    "my_out = torch.stack(my_out)\n",
    "my_out = my_out.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My out:\n",
      "tensor([[[-0.0941, -0.0626,  0.4790, -0.2041, -0.1372],\n",
      "         [ 0.2963, -0.1015,  0.0473,  0.3765, -0.0092],\n",
      "         [ 0.2581, -0.2344,  0.1297,  0.2794,  0.1541]],\n",
      "\n",
      "        [[-0.1746,  0.1412, -0.0872,  0.3289,  0.1479],\n",
      "         [-0.4669,  0.3670, -0.0742,  0.2993, -0.0806],\n",
      "         [-0.2444,  0.1372, -0.0368,  0.1408, -0.0089]]],\n",
      "       grad_fn=<PermuteBackward>)\n",
      "torch.Size([2, 3, 5])\n",
      "\n",
      "My out for linear layer:\n",
      "tensor([[-0.0941, -0.0626,  0.4790, -0.2041, -0.1372],\n",
      "        [ 0.2963, -0.1015,  0.0473,  0.3765, -0.0092],\n",
      "        [ 0.2581, -0.2344,  0.1297,  0.2794,  0.1541],\n",
      "        [-0.1746,  0.1412, -0.0872,  0.3289,  0.1479],\n",
      "        [-0.4669,  0.3670, -0.0742,  0.2993, -0.0806],\n",
      "        [-0.2444,  0.1372, -0.0368,  0.1408, -0.0089]], grad_fn=<ViewBackward>)\n",
      "torch.Size([6, 5])\n",
      "\n",
      "My out from linear layer:\n",
      "tensor([[0.2322],\n",
      "        [0.4353],\n",
      "        [0.3565],\n",
      "        [0.2144],\n",
      "        [0.1914],\n",
      "        [0.2213]], grad_fn=<AddmmBackward>)\n",
      "torch.Size([6, 1])\n",
      "\n",
      "Reshape my_out_from_lin to be batch_first:\n",
      "tensor([[0.2322, 0.4353, 0.3565],\n",
      "        [0.2144, 0.1914, 0.2213]], grad_fn=<ViewBackward>)\n",
      "torch.Size([2, 3])\n",
      "\n",
      "Final out:\n",
      "tensor([0.3565, 0.2213], grad_fn=<SelectBackward>)\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "# print()\n",
    "# print(\"Pytorch out:\")\n",
    "# print(out)\n",
    "\n",
    "# print()\n",
    "# print(\"Pytorch hiddens:\")\n",
    "# print(hx)\n",
    "\n",
    "print()\n",
    "print(\"My out:\")\n",
    "print(my_out)\n",
    "print(my_out.shape)\n",
    "\n",
    "fc = nn.Linear(rnn_hidden_size, 1)\n",
    "\n",
    "my_out_for_lin = my_out.contiguous().view(-1, rnn_hidden_size)\n",
    "print()\n",
    "print(\"My out for linear layer:\")\n",
    "print(my_out_for_lin)\n",
    "print(my_out_for_lin.shape)\n",
    "\n",
    "print()\n",
    "my_out_from_lin = fc(my_out_for_lin)\n",
    "print(\"My out from linear layer:\")\n",
    "print(my_out_from_lin)\n",
    "print(my_out_from_lin.shape)\n",
    "\n",
    "print()\n",
    "print(\"Reshape my_out_from_lin to be batch_first:\")\n",
    "my_out_semi_final = my_out_from_lin.view(batch_size, -1)\n",
    "print(my_out_semi_final)\n",
    "print(my_out_semi_final.shape)\n",
    "\n",
    "print()\n",
    "print(\"Final out:\")\n",
    "final_out = my_out_semi_final[:, -1]\n",
    "print(final_out)\n",
    "print(final_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
