{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH SIZE: 2\n",
      "SEQ LEN: 3\n",
      "EMB SIZE: 3\n",
      "RNN HIDDEN SIZE: 5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 3\n",
    "emb_size = 3\n",
    "rnn_hidden_size = 5\n",
    "print('BATCH SIZE:', batch_size)\n",
    "print(\"SEQ LEN:\", seq_len)\n",
    "print(\"EMB SIZE:\", emb_size)\n",
    "print(\"RNN HIDDEN SIZE:\", rnn_hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 8],\n",
      "        [6, 1],\n",
      "        [4, 4]])\n",
      "x_emb\n",
      "tensor([[[ 0.1124,  0.6408,  0.4412],\n",
      "         [-1.4689, -1.5867,  1.2032]],\n",
      "\n",
      "        [[-0.2159,  0.7924, -0.2897],\n",
      "         [ 0.4372,  0.4913, -0.2041]],\n",
      "\n",
      "        [[-0.4927,  0.2484,  0.4397],\n",
      "         [-0.4927,  0.2484,  0.4397]]], grad_fn=<EmbeddingBackward>)\n",
      "Pytorch results:\n",
      "out:\n",
      "tensor([[[ 0.3606,  0.1887, -0.1284, -0.0933, -0.0897,  0.2490, -0.0043,\n",
      "          -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.1044,  0.0042, -0.1612,  0.7877,  0.3083,  0.5819, -0.8315,\n",
      "          -0.0468,  0.9113, -0.0991]],\n",
      "\n",
      "        [[ 0.1415, -0.0160,  0.4295, -0.3774, -0.2531, -0.1587,  0.1502,\n",
      "           0.3547, -0.3648, -0.2646],\n",
      "         [ 0.2284,  0.3528, -0.2628, -0.2497, -0.1655, -0.3479,  0.2657,\n",
      "           0.1215, -0.4595, -0.0252]],\n",
      "\n",
      "        [[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415]]], grad_fn=<CatBackward>) torch.Size([3, 2, 10])\n",
      "\n",
      "hidden:\n",
      "tensor([[[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604]],\n",
      "\n",
      "        [[ 0.2490, -0.0043, -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.5819, -0.8315, -0.0468,  0.9113, -0.0991]]],\n",
      "       grad_fn=<StackBackward>) torch.Size([2, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "x_data = torch.randint(1, 9, (seq_len, batch_size))\n",
    "print(x_data)\n",
    "\n",
    "emb = nn.Embedding(9, emb_size)\n",
    "\n",
    "# for param in emb.parameters():\n",
    "#     print(param)\n",
    "\n",
    "x_emb = emb(x_data)\n",
    "\n",
    "print(\"x_emb\")\n",
    "print(x_emb)\n",
    "\n",
    "rnn = nn.RNN(emb_size, rnn_hidden_size, bidirectional=True, bias = False)\n",
    "\n",
    "# for name, param in rnn.named_parameters():\n",
    "#     print(name, param.shape)\n",
    "    \n",
    "# print(rnn.weight_hh_l0)\n",
    "# print()\n",
    "# print(rnn.weight_hh_l0_reverse)\n",
    "\n",
    "out, hidden = rnn(x_emb)\n",
    "print(\"Pytorch results:\")\n",
    "print(\"out:\")\n",
    "print(out, out.shape)\n",
    "print()\n",
    "print(\"hidden:\")\n",
    "print(hidden, hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output from forward pass:\n",
      "tensor([[[ 0.3606,  0.1887, -0.1284, -0.0933, -0.0897],\n",
      "         [ 0.1044,  0.0042, -0.1612,  0.7877,  0.3083]],\n",
      "\n",
      "        [[ 0.1415, -0.0160,  0.4295, -0.3774, -0.2531],\n",
      "         [ 0.2284,  0.3528, -0.2628, -0.2497, -0.1655]],\n",
      "\n",
      "        [[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604]]],\n",
      "       grad_fn=<StackBackward>)\n"
     ]
    }
   ],
   "source": [
    "tanh = nn.Tanh()\n",
    "\n",
    "wii = rnn.weight_ih_l0\n",
    "whh = rnn.weight_hh_l0\n",
    "\n",
    "seq_len, batch_size , _ = x_emb.shape\n",
    "\n",
    "outputs = []\n",
    "h_t_f = torch.zeros(batch_size, rnn_hidden_size)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    ii = x_emb[i] @ torch.transpose(wii, 0, 1)\n",
    "    hh = h_t_f @ torch.transpose(whh, 0, 1)\n",
    "    \n",
    "    h_t_f = tanh(ii + hh)\n",
    "    \n",
    "    outputs.append(h_t_f)\n",
    "    \n",
    "\n",
    "print()\n",
    "print(\"Output from forward pass:\")\n",
    "outputs = torch.stack(outputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from backward pass:\n",
      "tensor([[[ 0.2490, -0.0043, -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.5819, -0.8315, -0.0468,  0.9113, -0.0991]],\n",
      "\n",
      "        [[-0.1587,  0.1502,  0.3547, -0.3648, -0.2646],\n",
      "         [-0.3479,  0.2657,  0.1215, -0.4595, -0.0252]],\n",
      "\n",
      "        [[ 0.2642, -0.3012,  0.2351,  0.2848, -0.2415],\n",
      "         [ 0.2642, -0.3012,  0.2351,  0.2848, -0.2415]]],\n",
      "       grad_fn=<FlipBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_emb_reverse = torch.flip(x_emb, [0, 1])\n",
    "\n",
    "wii_reverse = rnn.weight_ih_l0_reverse\n",
    "whh_reverse = rnn.weight_hh_l0_reverse\n",
    "\n",
    "back_outputs = []\n",
    "h_t_b = torch.zeros(batch_size, rnn_hidden_size)\n",
    "\n",
    "for i in range(seq_len):\n",
    "    ii = x_emb_reverse[i] @ torch.transpose(wii_reverse, 0, 1)\n",
    "    hh = h_t_b @ torch.transpose(whh_reverse, 0, 1)\n",
    "    \n",
    "    h_t_b = tanh(ii + hh)\n",
    "    \n",
    "    back_outputs.append(h_t_b)\n",
    "\n",
    "    \n",
    "# print(\"raw back outputs:\")\n",
    "# print(back_outputs)\n",
    "# print()\n",
    "print(\"Output from backward pass:\")\n",
    "back_outputs = torch.flip(torch.stack(back_outputs), [0, 1])\n",
    "print(back_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output from forward pass:\n",
      "tensor([[[ 0.3606,  0.1887, -0.1284, -0.0933, -0.0897],\n",
      "         [ 0.1044,  0.0042, -0.1612,  0.7877,  0.3083]],\n",
      "\n",
      "        [[ 0.1415, -0.0160,  0.4295, -0.3774, -0.2531],\n",
      "         [ 0.2284,  0.3528, -0.2628, -0.2497, -0.1655]],\n",
      "\n",
      "        [[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "\n",
      "Output from backward pass:\n",
      "tensor([[[ 0.2490, -0.0043, -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.5819, -0.8315, -0.0468,  0.9113, -0.0991]],\n",
      "\n",
      "        [[-0.1587,  0.1502,  0.3547, -0.3648, -0.2646],\n",
      "         [-0.3479,  0.2657,  0.1215, -0.4595, -0.0252]],\n",
      "\n",
      "        [[ 0.2642, -0.3012,  0.2351,  0.2848, -0.2415],\n",
      "         [ 0.2642, -0.3012,  0.2351,  0.2848, -0.2415]]],\n",
      "       grad_fn=<FlipBackward>)\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(\"Output from forward pass:\")\n",
    "# outputs = torch.stack(outputs)\n",
    "print(outputs)\n",
    "\n",
    "print()\n",
    "print(\"Output from backward pass:\")\n",
    "# back_outputs = torch.flip(torch.stack(back_outputs), [0, 1])\n",
    "print(back_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch final output:\n",
      "tensor([[[ 0.3606,  0.1887, -0.1284, -0.0933, -0.0897,  0.2490, -0.0043,\n",
      "          -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.1044,  0.0042, -0.1612,  0.7877,  0.3083,  0.5819, -0.8315,\n",
      "          -0.0468,  0.9113, -0.0991]],\n",
      "\n",
      "        [[ 0.1415, -0.0160,  0.4295, -0.3774, -0.2531, -0.1587,  0.1502,\n",
      "           0.3547, -0.3648, -0.2646],\n",
      "         [ 0.2284,  0.3528, -0.2628, -0.2497, -0.1655, -0.3479,  0.2657,\n",
      "           0.1215, -0.4595, -0.0252]],\n",
      "\n",
      "        [[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415]]], grad_fn=<CatBackward>)\n",
      "\n",
      "my final output: \n",
      "tensor([[[ 0.3606,  0.1887, -0.1284, -0.0933, -0.0897,  0.2490, -0.0043,\n",
      "          -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.1044,  0.0042, -0.1612,  0.7877,  0.3083,  0.5819, -0.8315,\n",
      "          -0.0468,  0.9113, -0.0991]],\n",
      "\n",
      "        [[ 0.1415, -0.0160,  0.4295, -0.3774, -0.2531, -0.1587,  0.1502,\n",
      "           0.3547, -0.3648, -0.2646],\n",
      "         [ 0.2284,  0.3528, -0.2628, -0.2497, -0.1655, -0.3479,  0.2657,\n",
      "           0.1215, -0.4595, -0.0252]],\n",
      "\n",
      "        [[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604,  0.2642, -0.3012,\n",
      "           0.2351,  0.2848, -0.2415]]], grad_fn=<CatBackward>)\n"
     ]
    }
   ],
   "source": [
    "my_final_output = torch.cat((outputs, back_outputs), -1)\n",
    "print(\"pytorch final output:\")\n",
    "print(out)\n",
    "print()\n",
    "print(\"my final output: \")\n",
    "print(my_final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch final hidden:\n",
      "tensor([[[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797],\n",
      "         [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604]],\n",
      "\n",
      "        [[ 0.2490, -0.0043, -0.0176,  0.0240, -0.0313],\n",
      "         [ 0.5819, -0.8315, -0.0468,  0.9113, -0.0991]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "My final hidden:\n",
      "tensor([[ 0.2052, -0.1044,  0.2178,  0.2974, -0.0797],\n",
      "        [ 0.2302, -0.0262,  0.2849, -0.1412, -0.0604]], grad_fn=<TanhBackward>)\n",
      "tensor([[ 0.5819, -0.8315, -0.0468,  0.9113, -0.0991],\n",
      "        [ 0.2490, -0.0043, -0.0176,  0.0240, -0.0313]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(\"pytorch final hidden:\")\n",
    "print(hidden)\n",
    "\n",
    "print(\"My final hidden:\")\n",
    "print(h_t_f)\n",
    "print(h_t_b)\n",
    "# print(ht)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
