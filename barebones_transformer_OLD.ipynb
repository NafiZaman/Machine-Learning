{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following is a very simple transformer. It shows the inner workings of the architecture in detail while being very basic. Please look at the pytorch_transformers notebook for a better representation of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z1:\n",
      "tensor([[ 0.6614,  0.2669,  0.0617],\n",
      "        [ 0.6213, -0.4519, -0.1661],\n",
      "        [-1.5228,  0.3817, -1.0276]])\n",
      "z2:\n",
      "tensor([[-0.5631, -0.8923, -0.0583],\n",
      "        [-0.1955, -0.9656,  0.4224],\n",
      "        [ 0.2673, -0.4212, -0.5107]])\n",
      "z_concatenated:\n",
      "tensor([[ 0.6614,  0.2669,  0.0617, -0.5631, -0.8923, -0.0583],\n",
      "        [ 0.6213, -0.4519, -0.1661, -0.1955, -0.9656,  0.4224],\n",
      "        [-1.5228,  0.3817, -1.0276,  0.2673, -0.4212, -0.5107]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "z1 = torch.randn(3,3)\n",
    "\n",
    "print(\"z1:\")\n",
    "print(z1)\n",
    "\n",
    "z2 = torch.randn(3,3)\n",
    "print(\"z2:\")\n",
    "print(z2)\n",
    "\n",
    "z_concatenated = torch.cat((z1,z2), dim=1)\n",
    "print(\"z_concatenated:\")\n",
    "print(z_concatenated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider an input of 2 words and each word has 3 features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thinking', 'machines']\n",
      "{'thinking': 0, 'machines': 1}\n",
      "[0, 1]\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "sentence = [\"thinking\", \"machines\"]\n",
    "print(sentence)\n",
    "\n",
    "word2idx = {}\n",
    "\n",
    "for word in sentence:\n",
    "    if word not in word2idx:\n",
    "        word2idx[word] = len(word2idx)\n",
    "        \n",
    "print(word2idx)\n",
    "\n",
    "seq = list(word2idx.values())\n",
    "\n",
    "print(seq)\n",
    "\n",
    "seq_tensor = torch.tensor(seq, dtype=torch.long)\n",
    "print(seq_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Positional embeddings to feed into the Self-Attention layer in the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the positional embeddings:\n",
      "tensor([[ 0.6614,  1.2669,  0.0617],\n",
      "        [ 1.4628,  0.5481, -0.1661]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "def add_pos_encoding(embeddings, embed_len, seq_len):\n",
    "    pe = torch.zeros((seq_len, embed_len), dtype = torch.float32)\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        v1 = pe[pos]\n",
    "        for i in range(v1.shape[0]):\n",
    "            if i % 2 == 0:\n",
    "                pwr = (2 * i) / embed_len\n",
    "                v1[i] = np.sin(pos / (10000 ** pwr))\n",
    "            else:\n",
    "                pwr = (2 * i) / embed_len\n",
    "                v1[i] = np.cos(pos / (10000 ** pwr))\n",
    "                \n",
    "        pe[pos] = v1\n",
    "            \n",
    "#     print(pe)\n",
    "    \n",
    "    return embeddings + pe\n",
    "\n",
    "embedding_len = 3\n",
    "embeddings = nn.Embedding(len(word2idx), embedding_len)\n",
    "embeddings = embeddings(seq_tensor)\n",
    "# print(em)\n",
    "# print(embeddings)\n",
    "positional_embeddings = add_pos_encoding(embeddings, embed_len=embeddings.shape[1], seq_len=embeddings.shape[0])\n",
    "print(\"These are the positional embeddings:\")\n",
    "print(positional_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Multi-head Attention/ Self-Attention layer for the Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z1': tensor([[ 0.1160, -0.0776,  0.0271,  0.1079,  0.0828,  0.0779,  0.0888,  0.0791,\n",
      "          0.0323, -0.0855, -0.1050, -0.0573, -0.0146, -0.0766,  0.0457,  0.0387,\n",
      "         -0.0283,  0.0480,  0.0404,  0.0763,  0.0842, -0.0423,  0.1221, -0.0145,\n",
      "         -0.0043, -0.1180, -0.0805, -0.0730, -0.0535,  0.0889, -0.0408, -0.0934,\n",
      "          0.0481,  0.0400,  0.0810, -0.0647,  0.0271, -0.0455, -0.0281, -0.0996,\n",
      "         -0.0570, -0.0383,  0.0535,  0.0228,  0.0309,  0.1248,  0.1218,  0.0853,\n",
      "          0.0040, -0.0865,  0.0977, -0.0313, -0.0101, -0.1077, -0.0247, -0.0806,\n",
      "          0.1149, -0.1081, -0.0974, -0.0042, -0.0676,  0.0447, -0.0481, -0.0587]],\n",
      "       grad_fn=<AddBackward0>), 'z2': tensor([[-0.0237, -0.0461,  0.0522, -0.0426, -0.0702,  0.0284, -0.0904, -0.0267,\n",
      "          0.0639,  0.0467,  0.1064, -0.0853,  0.0011, -0.0369, -0.0699, -0.0703,\n",
      "          0.0315,  0.0131, -0.0638, -0.0881, -0.1036,  0.1225, -0.0213, -0.0289,\n",
      "         -0.0460, -0.0625, -0.1143, -0.0734,  0.0764,  0.0274, -0.0441, -0.0472,\n",
      "          0.0799,  0.0901,  0.1204,  0.0365,  0.0604, -0.0100, -0.0731, -0.1225,\n",
      "          0.0761, -0.0182,  0.0514,  0.0062, -0.1167, -0.0613,  0.0352, -0.0253,\n",
      "         -0.1207,  0.0438,  0.0048, -0.0402,  0.0505,  0.0264, -0.0461,  0.1123,\n",
      "          0.0627,  0.0157, -0.0240, -0.1059,  0.0965, -0.0323, -0.0808,  0.0801]],\n",
      "       grad_fn=<AddBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "queries = nn.ModuleList()\n",
    "keys = nn.ModuleList()\n",
    "values = nn.ModuleList()\n",
    "\n",
    "\n",
    "vector_dim = 64\n",
    "divide_by = np.sqrt(vector_dim)\n",
    "\n",
    "for i in range(positional_embeddings.shape[0]):\n",
    "    q = nn.Linear(vector_dim, 1, bias = False)\n",
    "    k = nn.Linear(vector_dim, 1)\n",
    "    v = nn.Linear(vector_dim, 1, bias = False)\n",
    "    \n",
    "    queries.append(q)\n",
    "    keys.append(k)\n",
    "    values.append(v)\n",
    "\n",
    "# calculate scores for each word:\n",
    "scores = {}\n",
    "for i in range(positional_embeddings.shape[0]):\n",
    "    \n",
    "    q_weight = queries[i].weight\n",
    "    scores_list = []\n",
    "    for j in range(len(keys)):\n",
    "        k_weight, k_bias = keys[j].weight, keys[j].bias\n",
    "        s = q_weight @ torch.transpose(k_weight, 0, 1) + k_bias\n",
    "        scores_list.append(s)\n",
    "    \n",
    "    scores_list = torch.stack(scores_list, dim=1).view(1, -1)\n",
    "    scores[\"word_\"+str(i+1)] = scores_list\n",
    "\n",
    "# divide them by the dimension of the vectors\n",
    "# and apply softmax\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "for word, tensors in scores.items():\n",
    "    tensors = softmax(tensors / divide_by)\n",
    "    tensors = tensors.squeeze(0).tolist()\n",
    "    scores[word] = tensors\n",
    "\n",
    "softmax_scores = list(scores.values())\n",
    "\n",
    "# multiply the softmax with the value vector of each word\n",
    "# for every word\n",
    "summed_values = {}\n",
    "\n",
    "for i in range(positional_embeddings.shape[0]):\n",
    "    vals = softmax_scores[i]\n",
    "    v_weight = values[i].weight\n",
    "    arr = []\n",
    "    for val in vals:\n",
    "        arr.append(val * v_weight)\n",
    "    \n",
    "    temp = 0\n",
    "    for tensor in arr:\n",
    "        temp = temp + tensor\n",
    "    summed_values[\"z\"+str(i+1)] = temp\n",
    "    \n",
    "print(summed_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Calculation of Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape: torch.Size([2, 64])\n"
     ]
    }
   ],
   "source": [
    "def get_self_attention(pos_embedds, embedding_size, vector_size, div_val):\n",
    "    # calculating q:\n",
    "    wq = nn.Linear(embedding_size, vector_size)\n",
    "    q = wq(pos_embedds)\n",
    "\n",
    "    # calculating k:\n",
    "    wk = nn.Linear(embedding_size, vector_size)\n",
    "    k = wk(pos_embedds)\n",
    "\n",
    "    # calculating v:\n",
    "    wv = nn.Linear(embedding_size, vector_size)\n",
    "    v = wv(pos_embedds)\n",
    "\n",
    "#     print(f\"q shape: {q.shape}, k shape:{k.shape}, v shape: {v.shape}\")\n",
    "\n",
    "    score = q @ torch.transpose(k, 0, 1)\n",
    "    score = softmax(score / div_val)\n",
    "    z = score @ v\n",
    "    \n",
    "    return z\n",
    "\n",
    "z = get_self_attention(positional_embeddings, embedding_len, vector_dim, divide_by)\n",
    "print(\"z shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beast with Many Heads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using the 'z' matrices computed above; we concatenate them and then pass them through a linear layer to get the final output of the Self-Attention layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output (z) shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "number_of_heads = 2\n",
    "z_matrices = {}\n",
    "\n",
    "for i in range(number_of_heads):\n",
    "    z = get_self_attention(positional_embeddings, embedding_len, vector_dim, divide_by)\n",
    "    z_matrices[\"z\"+str(i+1)] = z\n",
    "\n",
    "# print(z_matrices)\n",
    "\n",
    "list_of_zs = []\n",
    "for z in z_matrices.values():\n",
    "    list_of_zs.append(z)\n",
    "    \n",
    "z_tuple = tuple(list_of_zs)\n",
    "\n",
    "z_concatenated = torch.cat(z_tuple, dim=1)\n",
    "\n",
    "wo = nn.Linear(vector_dim*number_of_heads, embedding_len)\n",
    "z = wo(z_concatenated)\n",
    "\n",
    "print(\"Final output (z) shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add the output of the multi-headed attention layer to the positional embeddings (input into the M-HAL) and normalize it. This is called a risidual connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer norm output shape: torch.Size([2, 3])\n",
      "tensor([[-0.6960,  1.6577, -0.8487],\n",
      "        [ 0.5519,  0.5397, -1.2046]], grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "risidual_connection = z + positional_embeddings\n",
    "\n",
    "m = nn.LayerNorm([len(word2idx), embedding_len])\n",
    "\n",
    "layer_norm_output = m(risidual_connection)\n",
    "\n",
    "print(\"Layer norm output shape:\", layer_norm_output.shape)\n",
    "print(layer_norm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pointwise feed forward network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output:\n",
      "tensor([[-0.3710, -0.0980, -0.3630],\n",
      "        [-0.3306, -0.1015, -0.2085]], grad_fn=<AddmmBackward>)\n",
      "Shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "lin1 = nn.Linear(embedding_len, embedding_len)\n",
    "relu = nn.ReLU()\n",
    "lin2 = nn.Linear(embedding_len, embedding_len)\n",
    "\n",
    "encoder_output = lin1(layer_norm_output)\n",
    "encoder_output = relu(encoder_output)\n",
    "encoder_output = lin2(encoder_output)\n",
    "\n",
    "print(f\"Encoder output:\\n{encoder_output}\")\n",
    "print(\"Shape:\", encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6614,  1.2669,  0.0617],\n",
      "        [ 1.4628,  0.5481, -0.1661],\n",
      "        [-0.6135,  1.3817, -1.0276],\n",
      "        [-0.4219,  0.1077, -0.0582]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "target_sentence = [\"SOS\", \"tänkande\", \"maskiner\", \"EOS\"]\n",
    "target_tensor = torch.tensor([0,1,2,3], dtype=torch.long)\n",
    "\n",
    "vocab_size = 4\n",
    "embedding_len = 3\n",
    "vector_len = 64\n",
    "number_of_heads = 2\n",
    "divide_by = np.sqrt(vector_len)\n",
    "\n",
    "\n",
    "embeddings = nn.Embedding(vocab_size, embedding_len)\n",
    "embeddings = embeddings(target_tensor)\n",
    "# print(embeddings)\n",
    "# print()\n",
    "positional_embeddings = add_pos_encoding(embeddings, embed_len=embedding_len, seq_len=vocab_size)\n",
    "print(positional_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked multihead attention of a Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output (z) shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "def get_masked_attention(scores):\n",
    "    masked_attention = torch.zeros(scores.shape[0], scores.shape[1])\n",
    "    \n",
    "#     print(masked_attention)\n",
    "    for i in range(len(masked_attention)):\n",
    "        masked_attention[i, (i+1):] = -float('inf')\n",
    "        \n",
    "#     print(masked_attention)\n",
    "    \n",
    "    return masked_attention\n",
    "\n",
    "def get_masked_multihead_attention(positional_embeddings, vector_len, embedding_len, divide_by, mask):\n",
    "    wq = nn.Linear(embedding_len, vector_len)\n",
    "    q = wq(positional_embeddings)\n",
    "    \n",
    "    wk = nn.Linear(embedding_len, vector_len)\n",
    "    k = wk(positional_embeddings)\n",
    "    \n",
    "    wv = nn.Linear(embedding_len, vector_len)\n",
    "    v = wv(positional_embeddings)\n",
    "    \n",
    "    score = q @ torch.transpose(k, 0, 1)\n",
    "    \n",
    "    #scale the score\n",
    "    score = score / divide_by\n",
    "    \n",
    "    # before sending the score to the softmax layer\n",
    "    # we have to add the masked attention to it\n",
    "    if mask == True:\n",
    "        masked_attention = get_masked_attention(score)\n",
    "        score = score + masked_attention\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    score = softmax(score)\n",
    "        \n",
    "    z = score @ v\n",
    "    \n",
    "    return z\n",
    "    \n",
    "# z = get_masked_multihead_attention(positional_embeddings, vector_len, embedding_len, divide_by, True)\n",
    "# print(\"z shape:\", z.shape)\n",
    "\n",
    "z_matrices = {}\n",
    "\n",
    "for i in range(number_of_heads):\n",
    "    z = get_masked_multihead_attention(positional_embeddings, \n",
    "                                       vector_len, \n",
    "                                       embedding_len, \n",
    "                                       divide_by, \n",
    "                                       True)\n",
    "    z_matrices['z'+str(i+1)] = z\n",
    "    \n",
    "list_of_zs = []\n",
    "for z in z_matrices.values():\n",
    "    list_of_zs.append(z)\n",
    "    \n",
    "z_tuple = tuple(list_of_zs)\n",
    "\n",
    "z_concatenated = torch.cat(z_tuple, dim=1)\n",
    "\n",
    "wo = nn.Linear(vector_dim*number_of_heads, embedding_len)\n",
    "z = wo(z_concatenated)\n",
    "\n",
    "print(\"Final output (z) shape:\", z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and getting the normalized value of the output from the Masked MHAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer norm output shape: torch.Size([4, 3])\n",
      "tensor([[ 0.6209,  0.9641, -0.1326],\n",
      "        [ 1.8560, -0.0939, -0.6452],\n",
      "        [-0.9726,  1.4778, -1.6857],\n",
      "        [-0.8058, -0.3499, -0.2332]], grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "risidual_connection = z + positional_embeddings\n",
    "\n",
    "m = nn.LayerNorm([vocab_size, embedding_len])\n",
    "\n",
    "layer_norm_output = m(risidual_connection)\n",
    "\n",
    "print(\"Layer norm output shape:\", layer_norm_output.shape)\n",
    "print(layer_norm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder multihead attention output: tensor([[ 0.7282,  0.9485, -0.3348],\n",
      "        [ 1.8690, -0.0289, -0.8083],\n",
      "        [-0.7441,  1.4229, -1.7695],\n",
      "        [-0.5897, -0.2653, -0.4278]], grad_fn=<NativeLayerNormBackward>)\n",
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "def get_multihead_attention_decoder(q, k, v, div):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    score = q @ torch.transpose(k, 0, 1)\n",
    "    score = softmax(score / div)\n",
    "    \n",
    "    z = score @ v\n",
    "    \n",
    "    return z\n",
    "    \n",
    "# z = get_multihead_attention_decoder(layer_norm_output, encoder_output, encoder_output, divide_by)\n",
    "# print(\"z shape:\", z.shape)\n",
    "\n",
    "z_matrices = {}\n",
    "\n",
    "for i in range(number_of_heads):\n",
    "    z = get_multihead_attention_decoder(layer_norm_output, \n",
    "                                       encoder_output, \n",
    "                                       encoder_output, \n",
    "                                       divide_by)\n",
    "    z_matrices['z'+str(i+1)] = z\n",
    "    \n",
    "\n",
    "list_of_zs = []\n",
    "for z in z_matrices.values():\n",
    "    list_of_zs.append(z)\n",
    "    \n",
    "z_tuple = tuple(list_of_zs)\n",
    "\n",
    "z_concatenated = torch.cat(z_tuple, dim=1)\n",
    "\n",
    "# print(\"z_concatenated shape:\", z_concatenated.shape)\n",
    "\n",
    "wo = nn.Linear(z_concatenated.shape[1], embedding_len)\n",
    "z = wo(z_concatenated)\n",
    "\n",
    "# print(\"Final output (z) shape:\", z.shape)\n",
    "\n",
    "risidual_connection = z + layer_norm_output\n",
    "m = nn.LayerNorm([vocab_size, embedding_len])\n",
    "decoder_MHAL_output = m(risidual_connection)\n",
    "print(\"Decoder multihead attention output:\", decoder_MHAL_output)\n",
    "print(decoder_MHAL_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder feed forward network and AddNorm layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder FFNN output:\n",
      "tensor([[-0.3569, -0.0972, -0.2907],\n",
      "        [-0.3163, -0.0934,  0.1176],\n",
      "        [-0.3652, -0.0977, -0.3335],\n",
      "        [-0.3487, -0.0968, -0.2490]], grad_fn=<AddmmBackward>). Shape: torch.Size([4, 3])\n",
      "Decoder output:\n",
      "tensor([[ 0.5701,  1.0403, -0.4064],\n",
      "        [ 1.7275,  0.0866, -0.4703],\n",
      "        [-0.8804,  1.5046, -1.8538],\n",
      "        [-0.7130, -0.1484, -0.4567]], grad_fn=<NativeLayerNormBackward>). Shape: torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "lin1 = nn.Linear(embedding_len, embedding_len)\n",
    "relu = nn.ReLU()\n",
    "lin2 = nn.Linear(embedding_len, embedding_len)\n",
    "\n",
    "decoder_FF_output = lin1(decoder_MHAL_output)\n",
    "decoder_FF_output = relu(decoder_FF_output)\n",
    "decoder_FF_output = lin2(decoder_FF_output)\n",
    "\n",
    "print(f\"Decoder FFNN output:\\n{decoder_FF_output}. Shape: {decoder_FF_output.shape}\")\n",
    "\n",
    "risidual_connection = decoder_FF_output + decoder_MHAL_output\n",
    "m = nn.LayerNorm([vocab_size, embedding_len])\n",
    "decoder_output = m(risidual_connection)\n",
    "print(f\"Decoder output:\\n{decoder_output}. Shape: {decoder_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final steps of the decoder. Pass the output through a linear layer and then a softmax to get the probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1771, 0.0780, 0.4744, 0.2706]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "decoder_output_flattened = decoder_output.view(1, -1)\n",
    "# print(decoder_output_flattened.shape)\n",
    "\n",
    "lin = nn.Linear(decoder_output_flattened.shape[1], vocab_size)\n",
    "softmax = nn.Softmax(dim=1)\n",
    "probabilities = softmax(lin(decoder_output_flattened))\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpenv",
   "language": "python",
   "name": "nlpenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
