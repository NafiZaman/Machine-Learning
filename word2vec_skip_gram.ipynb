{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'dhaka is bangladesh capital'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['he', 'is', 'king'],\n",
       " ['she', 'is', 'a', 'queen'],\n",
       " ['he', 'is', 'a', 'man'],\n",
       " ['she', 'is', 'a', 'woman'],\n",
       " ['warsaw', 'is', 'poland', 'capital'],\n",
       " ['berlin', 'is', 'germany', 'capital'],\n",
       " ['dhaka', 'is', 'bangladesh', 'capital']]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_corpus(corpus):\n",
    "    tokens = [x.split() for x in corpus]\n",
    "    return tokens\n",
    "\n",
    "tokenized_corpus = tokenize_corpus(corpus)\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = []\n",
    "for sentence in tokenized_corpus:\n",
    "    for token in sentence:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary.append(token)\n",
    "            \n",
    "word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
    "idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(vocabulary)\n",
    "context_size = 2\n",
    "idx_pairs = []\n",
    "\n",
    "for sentence in tokenized_corpus:\n",
    "    indices = [word2idx[word] for word in sentence]\n",
    "    \n",
    "    # for each word, treat it as a center word\n",
    "    \n",
    "    for center_word_pos in range(len(indices)):\n",
    "        \n",
    "        # for each window position\n",
    "        \n",
    "        for w in range(-window_size, window_size + 1):\n",
    "            context_word_pos = center_word_pos + w\n",
    "            \n",
    "            # make sure not to jump outside of the sentence\n",
    "            \n",
    "            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:\n",
    "                continue\n",
    "            context_word_idx = indices[context_word_pos]\n",
    "            idx_pairs.append((indices[center_word_pos], context_word_idx))\n",
    "\n",
    "idx_pairs = np.array(idx_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(SkipGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "    \n",
    "    def forward(self, word_index):\n",
    "        embeds = self.embeddings(word_index)\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "skip_gram_model = SkipGramLanguageModeler(vocabulary_size, embedding_dims, context_size)\n",
    "optimizer = optim.SGD(skip_gram_model.parameters(), lr = 1e-2)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    total_loss = 0\n",
    "    for word, target in idx_pairs:\n",
    "        word_idx = torch.tensor([word], dtype=torch.long)\n",
    "        \n",
    "        skip_gram_model.zero_grad()\n",
    "        \n",
    "        log_probs = skip_gram_model(word_idx)\n",
    "        \n",
    "        foo = torch.tensor([target], dtype=torch.long)\n",
    "        \n",
    "#         print(f'tens:{foo}, shape: {foo.shape}')\n",
    "        \n",
    "        loss = loss_function(log_probs, foo)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(TestModeler, self).__init__()\n",
    "        self.linear1 = nn.Linear(vocab_size, embedding_dim)\n",
    "        self.linear2 = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, word_idx, vocab_size):\n",
    "        x = torch.zeros(vocab_size).float()\n",
    "        x[word_idx] = 1.0\n",
    "        \n",
    "        out = self.linear1(x)\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=0)\n",
    "        return log_probs.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "test_model = TestModeler(vocabulary_size, 5)\n",
    "optimizer = optim.SGD(test_model.parameters(), lr=1e-2)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss_val = 0\n",
    "    \n",
    "    for data, target in idx_pairs:\n",
    "        \n",
    "        test_model.zero_grad()\n",
    "        log_probs = test_model(data, vocabulary_size)\n",
    "        foo = torch.tensor([target], dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        loss = loss_function(log_probs, foo)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_val+=loss.item()\n",
    "        \n",
    "    test_losses.append(loss_val)\n",
    "#     if epoch % 100 == 0:\n",
    "#         print(f'Loss at epoch {epoch}: {loss_val/len(idx_pairs)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of skip gram:  germany\n",
      "Prediction of test model:  germany\n"
     ]
    }
   ],
   "source": [
    "word = 'berlin'\n",
    "a = skip_gram_model(torch.tensor([word2idx[word]],dtype=torch.long))\n",
    "b = test_model(word2idx[word],vocabulary_size)\n",
    "print(\"Prediction of skip gram: \", idx2word[torch.argmax(a[0]).item()])\n",
    "print(\"Prediction of test model: \", idx2word[torch.argmax(b[0]).item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "pytorchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
